{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acknowledged-tuesday",
   "metadata": {},
   "source": [
    "## 10. 分散表現(word2vec)\n",
    "#### 推論ベースの手法\n",
    "推論ベースの手法では、周囲の単語が与えられたときに「?」にどのような単語が出現するかを推測する。\n",
    "\n",
    "'''\n",
    "you ? goodbye and i say hello.\n",
    "'''\n",
    "\n",
    "このとき、推論モデルは\\[you, goodbye\\]というコンテキストを入力として与えられたときに、  \n",
    "各単語の出現確率を出力する。この場合、\\[say\\]の出現確率が最も高くなるように学習される。  \n",
    "  \n",
    "ニューラルネットワークで単語の処理を行うため、単語を「固定長のベクトル」に変換する必要がある。  \n",
    "その方法の1つとして**one-hot表現**がある。これは、要素の中で1つだけ1で、残りはすべて0であるようなベクトルのことである。  \n",
    "例えば 「you」の単語IDが2のとき、one-hot表現は$(0,0,1,0,0,0,0)$のようになる。  \n",
    "  \n",
    "ここで、全結合層を考えると入力$\\boldsymbol{c}$(one-hot表現)と重み行列$\\boldsymbol{W}$の行列の積の計算となる。  \n",
    "このとき出力は、重み行列から該当する場所の行ベクトルを抜き出しただけの値となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "threaded-twins",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fb24c74d1406>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMatMul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "c = np.array([1,0,0,0,0,0,0])\n",
    "W = np.random.randn(7, 3)\n",
    "layer = MatMul(W)\n",
    "h = layer.forward(c)\n",
    "print(W)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-perspective",
   "metadata": {},
   "source": [
    "#### CBOW(Continuous Bag-of-Words)\n",
    "word2vecでは、CBOWモデルとskip-gramモデルの2つのモデルが使用されるニューラルネットワークである。  \n",
    "**CBOWモデル**は、コンテキストからターゲットを推測することを目的としたニューラルネットワークのことである。  \n",
    "  \n",
    "CBOWモデルは、入力層が2つあり(コンテキストの数)、中間層を経て、出力層へたどり着く。  \n",
    "入力層から中間層へは、同じ全結合層($\\boldsymbol{W_{in}}$)によって行われ、中間層から出力層へは全結合層($\\boldsymbol{W_{out}}$)によって行われる。  \n",
    "全結合層により、1つ目の入力層が$\\boldsymbol{h_{1}}$、2つ目の入力層が$\\boldsymbol{h_{2}}$に変換されたとすると、  \n",
    "中間層のニューロンは$\\frac{1}{2}(\\boldsymbol{h_{1}}+\\boldsymbol{h_{2}})$となる。  \n",
    "出力層には単語の数だけニューロンが存在するが、出力されたスコアにSoftmax関数を適用することで確率が得られる。  \n",
    "  \n",
    "全結合層の重み$\\boldsymbol{W_{in}}$は、単語の分散表現となる。  \n",
    "中間層のニューロンの数を入力層よりも減らすことで、単語を予測するために必要な情報をコンパクトに納められる。  \n",
    "これは**エンコード**と呼ばれ、中間層から目的の結果を得る作業は**デコード**と呼ばれる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "advanced-imaging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.13586867 -3.06723747 -0.19911345 -0.04935749  0.94521135 -0.1560266\n",
      "  0.54348746]\n"
     ]
    }
   ],
   "source": [
    "c0 = np.array([1,0,0,0,0,0,0])\n",
    "c1 = np.array([0,0,1,0,0,0,0])\n",
    "\n",
    "W_in = np.random.randn(7, 3)\n",
    "W_out = np.random.randn(3, 7)\n",
    "\n",
    "in_layer0 = MatMul(W_in)\n",
    "in_layer1 = MatMul(W_in)\n",
    "out_layer = MatMul(W_out)\n",
    "\n",
    "h0 = in_layer0.forward(c0)\n",
    "h1 = in_layer0.forward(c1)\n",
    "h = 0.5 * (h0 + h1)\n",
    "s = out_layer.forward(h)\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-georgia",
   "metadata": {},
   "source": [
    "#### CBOWモデルの学習\n",
    "CBOWモデルの学習は、正しい予測ができるように重みを調整することである。  \n",
    "CBOWモデルはコーパスにおける単語の出現パターンを学ぶだけであり、コーパスが違えば学習で得られる単語の分散表現も異なる。  \n",
    "$\\boldsymbol{W_{out}}$についても、単語の意味がエンコードされたベクトルが格納されていると考えられるが、  \n",
    "入力側の$\\boldsymbol{W_{in}}$だけを利用するのが一般的である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lesbian-attraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 1, 5, 6]\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(corpus)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "developed-battery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contexts_target(corpus, window_size=1):\n",
    "    # windowサイズに合わせてtargetの範囲を決める\n",
    "    target = corpus[window_size:-window_size]\n",
    "    contexts = []\n",
    "    \n",
    "    # targetのID\n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = [] # targetごとにコンテキストのリストを作成\n",
    "        # 左隣から右隣まで\n",
    "        for t in range(-window_size, window_size+1):\n",
    "            # t=0は自分自身\n",
    "            if t == 0:\n",
    "                continue\n",
    "            # IDをcsに追加\n",
    "            cs.append(corpus[idx+t])\n",
    "        # IDリストをcontextsに追加\n",
    "        contexts.append(cs)\n",
    "    \n",
    "    return np.array(contexts), np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "medical-matter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]]\n",
      "[1 2 3 4 1 5]\n"
     ]
    }
   ],
   "source": [
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "print(contexts)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "present-courtesy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_one_hot(corpus, vocab_size):\n",
    "    '''one-hot表現への変換\n",
    "\n",
    "    :param corpus: 単語IDのリスト（1次元もしくは2次元のNumPy配列）\n",
    "    :param vocab_size: 語彙数\n",
    "    :return: one-hot表現（2次元もしくは3次元のNumPy配列）\n",
    "    '''\n",
    "    N = corpus.shape[0]\n",
    "    \n",
    "    # コーパスが1次元配列\n",
    "    if corpus.ndim == 1:\n",
    "        # コーパスの各単語(N)に対し,単語数(vocab_size)の列を用意\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "        # コーパスを順に処理\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            # 該当する単語の位置に1を入れる\n",
    "            one_hot[idx, word_id] = 1\n",
    "            \n",
    "    # コーパスが多次元(C次元)\n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        # コーパスの各単語(N)に対し,C次元分(コンテキスト数)の単語数(vocab_size)の列を用意\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        # 1番目の単語から処理していく\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            # 次元ごとに処理していく\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "substantial-routine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]]\n",
      "[[[1 0 0 0 0 0 0]\n",
      "  [0 0 1 0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0 0 0 0]\n",
      "  [0 0 0 1 0 0 0]]\n",
      "\n",
      " [[0 0 1 0 0 0 0]\n",
      "  [0 0 0 0 1 0 0]]\n",
      "\n",
      " [[0 0 0 1 0 0 0]\n",
      "  [0 1 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 1 0 0]\n",
      "  [0 0 0 0 0 1 0]]\n",
      "\n",
      " [[0 1 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 1]]]\n"
     ]
    }
   ],
   "source": [
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts =  convert_one_hot(contexts, vocab_size)\n",
    "print(target)\n",
    "print(contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-helmet",
   "metadata": {},
   "source": [
    "#### CBOWモデルの実装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "systematic-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCBOW:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        \n",
    "        # 重みの初期化\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "        \n",
    "        # レイヤの生成\n",
    "        self.in_layer0 = MatMul(W_in)\n",
    "        self.in_layer1 = MatMul(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "        \n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            # 同じ重みを複数のレイヤで共有していることに注意\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        \n",
    "        # メンバ変数に単語の分散表現を設定\n",
    "        self.word_vecs = W_in\n",
    "    \n",
    "    def forward(self, contexts, target):\n",
    "        h0 = self.in_layer0.forward(contexts[:, 0]) # 1番目の次元はコンテキストのウィンドウサイズ分\n",
    "        h1 = self.in_layer1.forward(contexts[:, 1])\n",
    "        h = (h0 + h1) * 0.5\n",
    "        score = self.out_layer.forward(h)\n",
    "        loss = self.loss_layer.forward(score, target)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        ds = self.loss_layer.backward(dout)\n",
    "        da = self.out_layer.backward(ds)\n",
    "        da *= 0.5 # 「×」の逆伝播\n",
    "        self.in_layer1.backward(da)\n",
    "        self.in_layer0.backward(da)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-minutes",
   "metadata": {},
   "source": [
    "#### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "internal-darkness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 101 |  iter 1 / 2 | time 0[s] | loss 1.74\n",
      "| epoch 201 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
      "| epoch 301 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 401 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 501 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 601 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 701 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 801 |  iter 1 / 2 | time 0[s] | loss 0.32\n",
      "| epoch 901 |  iter 1 / 2 | time 0[s] | loss 0.49\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "window_size = 1\n",
    "hidden_size = 5\n",
    "batch_size = 3\n",
    "max_epoch = 1000\n",
    "eval_interval = 200\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts =  convert_one_hot(contexts, vocab_size)\n",
    "\n",
    "model = SimpleCBOW(vocab_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "trainer.fit(contexts, target, max_epoch, batch_size, eval_interval=100)\n",
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "opposed-duration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you [-1.5453827 -1.1059792 -1.1511161 -0.5819494  1.1674844]\n",
      "say [-0.5813489   1.1815686   1.1846436   0.03516261 -1.1850824 ]\n",
      "goodbye [-0.35639533 -0.79218745 -0.7693759   1.479449    0.6887518 ]\n",
      "and [-1.6112465   0.9196267   0.93049693 -1.5126876  -0.9213449 ]\n",
      "i [-0.3651697  -0.77786314 -0.76457274  1.4934592   0.68853694]\n",
      "hello [-1.5454439 -1.1294364 -1.1397979 -0.584809   1.1681837]\n",
      ". [ 1.4954163  1.101779   1.0781047  1.3852026 -1.0819888]\n"
     ]
    }
   ],
   "source": [
    "word_vecs = model.word_vecs\n",
    "\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word, word_vecs[word_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-reconstruction",
   "metadata": {},
   "source": [
    "$A$という事象が起こる確率を$P(A)$と書き、「$A$と$B$が同時に起こる確率」は**同時確率**と呼び$P(A,B)$とかく。  \n",
    "また、「$B$という情報が与えられたときに、$A$が起こる確率」を**事後確率**と呼び$P(A|B)$と表す。  \n",
    "$w_1,w_2,\\cdots,w_T$という単語の列で表されるコーパスを扱うとすると、  \n",
    "コンテキストとして$w_{t-1}$、$w_{t+1}$が与えられたときに、ターゲットが$w_t$となる確率は、\n",
    "$$\n",
    "P(w_t|w_{t-1},w_{t+1})\n",
    "$$\n",
    "と書くことができる。  \n",
    "**負の対数尤度**は、\n",
    "$$\n",
    "L=-\\log{P(w_t|w_{t-1},w_{t+1})}\n",
    "$$\n",
    "と書くことができて、コーパス全体に拡張すると、\n",
    "$$\n",
    "L=-\\frac{1}{T}\\sum_{t=1}^T\\log{P(w_t|w_{t-1},w_{t+1})}\n",
    "$$\n",
    "と書ける。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-steam",
   "metadata": {},
   "source": [
    "#### skip-gramモデル\n",
    "CBOWで扱うコンテキストとターゲットを逆転させたモデルで、  \n",
    "中央の単語(ターゲット)から、周囲の複数ある単語(コンテキスト)を推測する。  \n",
    "確率でskip-gramモデルを表すと、「$w_t$が与えられたときに、$w_{t-1}$と$w_{t+1}$が同時に起こる確率」となるので\n",
    "$$\n",
    "P(w_{t-1},w_{t+1}|w_t)\n",
    "$$\n",
    "と書ける。  \n",
    "コンテキストの単語の間に関連性がないと仮定すると、\n",
    "$$\n",
    "P(w_{t-1},w_{t+1}|w_t)=P(w_{t-1}|w_t)P(w_{t+1}|w_t)\n",
    "$$\n",
    "損失関数は交差エントロピー誤差に適用することで、\n",
    "$$\n",
    "L=-\\log{P(w_{t-1},w_{t+1}|w_t)}=-(\\log{P(w_{t-1}|w_t)}+\\log{P(w_{t+1}|w_t)})\n",
    "$$\n",
    "コーパス全体に拡張すると、\n",
    "$$\n",
    "L=-\\frac{1}{T}\\sum_{t=1}^T(\\log{P(w_{t-1}|w_t)}+\\log{P(w_{t+1}|w_t)})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dedicated-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSkipGram:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 重みの初期化\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.in_layer = MatMul(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer1 = SoftmaxWithLoss()\n",
    "        self.loss_layer2 = SoftmaxWithLoss()\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        layers = [self.in_layer, self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # メンバ変数に単語の分散表現を設定\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h = self.in_layer.forward(target)\n",
    "        s = self.out_layer.forward(h)\n",
    "        l1 = self.loss_layer1.forward(s, contexts[:, 0])\n",
    "        l2 = self.loss_layer2.forward(s, contexts[:, 1])\n",
    "        loss = l1 + l2\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dl1 = self.loss_layer1.backward(dout)\n",
    "        dl2 = self.loss_layer2.backward(dout)\n",
    "        ds = dl1 + dl2\n",
    "        dh = self.out_layer.backward(ds)\n",
    "        self.in_layer.backward(dh)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "combined-margin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 2 | time 0[s] | loss 3.89\n",
      "| epoch 101 |  iter 1 / 2 | time 0[s] | loss 3.50\n",
      "| epoch 201 |  iter 1 / 2 | time 0[s] | loss 2.72\n",
      "| epoch 301 |  iter 1 / 2 | time 0[s] | loss 2.23\n",
      "| epoch 401 |  iter 1 / 2 | time 0[s] | loss 2.02\n",
      "| epoch 501 |  iter 1 / 2 | time 0[s] | loss 1.77\n",
      "| epoch 601 |  iter 1 / 2 | time 0[s] | loss 2.18\n",
      "| epoch 701 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 801 |  iter 1 / 2 | time 0[s] | loss 2.13\n",
      "| epoch 901 |  iter 1 / 2 | time 0[s] | loss 1.67\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "window_size = 1\n",
    "hidden_size = 5\n",
    "batch_size = 3\n",
    "max_epoch = 1000\n",
    "eval_interval = 200\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts =  convert_one_hot(contexts, vocab_size)\n",
    "\n",
    "model = SimpleSkipGram(vocab_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "trainer.fit(contexts, target, max_epoch, batch_size, eval_interval=100)\n",
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "surface-jones",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you [-0.01092086  0.01764772 -0.00387958 -0.01076507  0.00539771]\n",
      "say [ 0.6594897  -1.1946642   0.8891292  -0.8312822   0.40401164]\n",
      "goodbye [-0.87017876  0.7940814  -0.78657174  0.8395835  -1.3091013 ]\n",
      "and [ 1.1279892  0.0731364  0.9573826 -1.0665976 -1.3647411]\n",
      "i [-0.88903165  0.797176   -0.7941236   0.8051016  -1.307444  ]\n",
      "hello [-0.5828587   1.0299876  -0.94866025  0.82907915  1.2038207 ]\n",
      ". [0.00564241 0.01827684 0.00389403 0.01849347 0.00568972]\n"
     ]
    }
   ],
   "source": [
    "word_vecs = model.word_vecs\n",
    "\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word, word_vecs[word_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-brief",
   "metadata": {},
   "source": [
    "#### Embeddingレイヤ\n",
    "入力の単語のone-hotベクトルと重み行列の積は、行列から該当する行を抜き出すことに相当することであった。  \n",
    "MatMulレイヤでなく、単語IDに該当する行を抜き出すためのレイヤとして**Embeddingレイヤ**を作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "difficult-madrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.grads = None\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0 # dWの要素を0で上書き\n",
    "        \n",
    "        for i, word_id in enumerate(self.idx):\n",
    "            # 該当する行にのみ勾配を伝える\n",
    "            dW[word_id] += dout[i] # IDが重複した場合は加算していく\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-thermal",
   "metadata": {},
   "source": [
    "#### Negative sampling\n",
    "今までは各単語の確率を求める「多値分類」を行ってきたが、「目的の単語であるかどうか」の「2値分類」を考えることにする。  \n",
    "中間層から出力層に向かうとき、重み行列の目的となる単語に対応する列ベクトルのみを掛け合わせる処理を行う。  \n",
    "2値分類の場合、出力層にはSoftmax関数、損失関数には交差エントロピー誤差を用いる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W)\n",
    "        self.params = self.embed.params\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, h, idx):\n",
    "        # 該当する行を抜き出す\n",
    "        target_W = self.embed.forward(idx)\n",
    "        # 中間層からの出力と掛け合わせ、各要素を足し合わせる\n",
    "        out = np.sum(target_W * h, axis=1)\n",
    "        \n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0], 1)\n",
    "        \n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-karaoke",
   "metadata": {},
   "source": [
    "いままでの方法であると正例のみ学習させることになるため、負例もいくつかサンプリングして損失を求める。  \n",
    "この手法を**Negative Sampling**と呼ぶ。  \n",
    "計算としては、各単語の損失を足し合わせて最終的な損失とするだけである。  \n",
    "また、負例のサンプリングについては単語の出現回数をもとに頻度が高いものが選ばれやすくする。  \n",
    "また、Nrgative samplingでは、確率分布に対して以下のように0.75乗する工夫がされている(低確率の単語が少しだけサンプリングされやすくなる)。\n",
    "$$\n",
    "P'(w_i)=\\frac{P(w_i)^{0.75}}{\\sum_{j}^nP(w_j)^{0.75}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "compact-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "GPU = False\n",
    "\n",
    "class UnigramSampler:\n",
    "    def __init__(self, corpus, power, sample_size):\n",
    "        self.sample_size = sample_size\n",
    "        self.vocab_size = None\n",
    "        self.word_p = None\n",
    "\n",
    "        counts = collections.Counter()\n",
    "        for word_id in corpus:\n",
    "            counts[word_id] += 1\n",
    "\n",
    "        vocab_size = len(counts)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.word_p = np.zeros(vocab_size)\n",
    "        for i in range(vocab_size):\n",
    "            self.word_p[i] = counts[i]\n",
    "\n",
    "        self.word_p = np.power(self.word_p, power)\n",
    "        self.word_p /= np.sum(self.word_p)\n",
    "\n",
    "    def get_negative_sample(self, target):\n",
    "        batch_size = target.shape[0]\n",
    "\n",
    "        if not GPU:\n",
    "            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                p = self.word_p.copy()\n",
    "                target_idx = target[i]\n",
    "                p[target_idx] = 0\n",
    "                p /= p.sum()\n",
    "                negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)\n",
    "        else:\n",
    "            # GPU(cupy）で計算するときは、速度を優先\n",
    "            # 負例にターゲットが含まれるケースがある\n",
    "            negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),\n",
    "                                               replace=True, p=self.word_p)\n",
    "\n",
    "        return negative_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "hundred-crest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 2]\n",
      " [0 4]\n",
      " [1 3]]\n"
     ]
    }
   ],
   "source": [
    "# 負例のサンプリング\n",
    "corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])\n",
    "power = 0.75\n",
    "sample_size = 2\n",
    "\n",
    "sampler = UnigramSampler(corpus, power, sample_size)\n",
    "target = np.array([1, 3, 0])\n",
    "negative_sample = sampler.get_negative_sample(target)\n",
    "print(negative_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-sender",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSamplingLoss:\n",
    "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "        # 正例1つ、負例のレイヤをsample_size個だけ生成\n",
    "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
    "        \n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "    \n",
    "    def forward(self, h, target):\n",
    "        # h: 中間層のニューロン\n",
    "        batch_size = target.shape[0]\n",
    "        # 負例のサンプリング\n",
    "        negative_sample = self.sampler.get_negative_sample(target)\n",
    "        \n",
    "        # 正例のフォワード\n",
    "        score = self.embed_dot_layers[0].forward(h, target)\n",
    "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
    "        loss = self.loss_layers[0].forward(score, correct_label)\n",
    "        \n",
    "        # 負例のフォワード\n",
    "        # 正解ラベルは0\n",
    "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
    "        for i in range(self.sample_size):\n",
    "            negative_target = negative_sample[:, i]\n",
    "            score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
    "            loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dh = 0\n",
    "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
    "            dscore = l0.backward(dout)\n",
    "            dh += l1.backward(dscore)\n",
    "        \n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-explosion",
   "metadata": {},
   "source": [
    "#### 改良版word2vecの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        \n",
    "        # 重みの初期化\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        \n",
    "        # レイヤの生成\n",
    "        self.in_layers = []\n",
    "        for i in range(2 * window_size):\n",
    "            layer = Embedding(W_in)\n",
    "            self.in_layers.append(layer)\n",
    "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
    "        \n",
    "        # すべての重みと勾配を配列にまとめる\n",
    "        layers = self.in_layers + [self.ns_loss]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        \n",
    "        self.word_vecs = W_in\n",
    "    \n",
    "    def forward(self, contexts, target):\n",
    "        h = 0\n",
    "        for i, layer in enumerate(self.in_layers):\n",
    "            h += layer.forward(contexts[:, i])\n",
    "        h *= 1 / len(self.in_layers)\n",
    "        loss = self.ns_loss.forward(h, target)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dout = self.ns_loss.backward(dout)\n",
    "        dout *= 1 / len(self.in_layers)\n",
    "        for layer in self.in_layers:\n",
    "            layer.backward(dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "governmental-robin",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ptb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-c662ef6d63bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmax_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_to_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_to_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mptb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloda_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_to_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ptb' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "window_size = 5\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "max_epoch = 10\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.loda_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "if config.GPU:\n",
    "    contexts, target = to_gpu(contexts), to_gpu(target)\n",
    "\n",
    "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()\n",
    "\n",
    "word_vecs = model.word_vecs\n",
    "\n",
    "if config.GPU:\n",
    "    word_vecs = to_cpu(word_vecs)\n",
    "params = {}\n",
    "params['word_vecs'] = word_vecs.astype(np.float32)\n",
    "params['word_to_id'] = word_to_id\n",
    "params['id_to_word'] = id_to_word\n",
    "pkl_file = 'cbow_params.pkl'\n",
    "with open(pkl_file, 'wb') as f:\n",
    "    pickle.dump(params, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "instructional-preference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " we: 0.6103515625\n",
      " someone: 0.59130859375\n",
      " i: 0.55419921875\n",
      " something: 0.48974609375\n",
      " anyone: 0.47314453125\n",
      "\n",
      "[query] year\n",
      " month: 0.71875\n",
      " week: 0.65234375\n",
      " spring: 0.62744140625\n",
      " summer: 0.6259765625\n",
      " decade: 0.603515625\n",
      "\n",
      "[query] car\n",
      " luxury: 0.497314453125\n",
      " arabia: 0.47802734375\n",
      " auto: 0.47119140625\n",
      " disk-drive: 0.450927734375\n",
      " travel: 0.4091796875\n",
      "\n",
      "[query] toyota\n",
      " ford: 0.55078125\n",
      " instrumentation: 0.509765625\n",
      " mazda: 0.49365234375\n",
      " bethlehem: 0.47509765625\n",
      " nissan: 0.474853515625\n"
     ]
    }
   ],
   "source": [
    "pkl_file = 'C:/Users/tanak/study/参考書/ゼロからDL2/deep-learning-from-scratch-2-master/ch04/cbow_params.pkl'\n",
    "\n",
    "with open(pkl_file, 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "    word_vecs = params['word_vecs']\n",
    "    word_to_id = params['word_to_id']\n",
    "    id_to_word = params['id_to_word']\n",
    "\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-temperature",
   "metadata": {},
   "source": [
    "「king - man + woman = queen」で有名な**アナロジー問題**という類似問題を、分散表現の加算と減算で解くことができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "sized-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(a, b, c, word_to_id, id_to_word, word_matrix, top=5, answer=None):\n",
    "    for word in (a, b, c):\n",
    "        if word not in word_to_id:\n",
    "            print('%s is not found' % word)\n",
    "            return\n",
    "\n",
    "    print('\\n[analogy] ' + a + ':' + b + ' = ' + c + ':?')\n",
    "    a_vec, b_vec, c_vec = word_matrix[word_to_id[a]], word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]\n",
    "    query_vec = b_vec - a_vec + c_vec\n",
    "    query_vec = normalize(query_vec)\n",
    "\n",
    "    similarity = np.dot(word_matrix, query_vec)\n",
    "\n",
    "    if answer is not None:\n",
    "        print(\"==>\" + answer + \":\" + str(np.dot(word_matrix[word_to_id[answer]], query_vec)))\n",
    "\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if np.isnan(similarity[i]):\n",
    "            continue\n",
    "        if id_to_word[i] in (a, b, c):\n",
    "            continue\n",
    "        print(' {0}: {1}'.format(id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return\n",
    "    \n",
    "def normalize(x):\n",
    "    if x.ndim == 2:\n",
    "        s = np.sqrt((x * x).sum(1))\n",
    "        x /= s.reshape((s.shape[0], 1))\n",
    "    elif x.ndim == 1:\n",
    "        s = np.sqrt((x * x).sum())\n",
    "        x /= s\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "finnish-bracket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy] king:man = queen:?\n",
      " woman: 5.16015625\n",
      " veto: 4.9296875\n",
      " ounce: 4.69140625\n",
      " earthquake: 4.6328125\n",
      " successor: 4.609375\n"
     ]
    }
   ],
   "source": [
    "analogy('king', 'man', 'queen', word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-letter",
   "metadata": {},
   "source": [
    "分散表現については、ゼロから学習することはなく、先に大きなコーパス(Wikipediaなど)で学習を行い、  \n",
    "その学習済みの分散表現を個別タスクで利用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-laundry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "precious-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# 前処理\n",
    "def preprocess(text):\n",
    "    # 前処理\n",
    "    text = text.lower() # すべての文字を小文字に変換\n",
    "    text = text.replace('.', ' .') # 「.」も1単語として扱うためスペースを入れる。\n",
    "    words = text.split(' ')\n",
    "    \n",
    "    # 単語辞書、ID辞書の作成\n",
    "    word_to_id = {} # 単語: ID\n",
    "    id_to_word = {} # ID: 単語\n",
    "\n",
    "    for word in words: # 単語のリスト\n",
    "        if word not in word_to_id: # word_to_idに存在しない場合\n",
    "            new_id = len(word_to_id) # 新しいIDの発行\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "    \n",
    "    # コーパスの作成(単語リストをIDリストに変換)\n",
    "    corpus = [word_to_id[w] for w in words]\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "# 関数\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "def clip_grads(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate\n",
    "\n",
    "def remove_duplicate(params, grads):\n",
    "    '''\n",
    "    パラメータ配列中の重複する重みをひとつに集約し、\n",
    "    その重みに対応する勾配を加算する\n",
    "    '''\n",
    "    params, grads = params[:], grads[:]  # copy list\n",
    "\n",
    "    while True:\n",
    "        find_flg = False\n",
    "        L = len(params)\n",
    "\n",
    "        for i in range(0, L - 1):\n",
    "            for j in range(i + 1, L):\n",
    "                # 重みを共有する場合\n",
    "                if params[i] is params[j]:\n",
    "                    grads[i] += grads[j]  # 勾配の加算\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "                # 転置行列として重みを共有する場合（weight tying）\n",
    "                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n",
    "                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n",
    "                    grads[i] += grads[j].T\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "\n",
    "                if find_flg: break\n",
    "            if find_flg: break\n",
    "\n",
    "        if not find_flg: break\n",
    "\n",
    "    return params, grads\n",
    "\n",
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    '''コサイン類似度の算出\n",
    "\n",
    "    :param x: ベクトル\n",
    "    :param y: ベクトル\n",
    "    :param eps: ”0割り”防止のための微小値\n",
    "    :return:\n",
    "    '''\n",
    "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
    "    return np.dot(nx, ny)\n",
    "\n",
    "\n",
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    '''類似単語の検索\n",
    "\n",
    "    :param query: クエリ（テキスト）\n",
    "    :param word_to_id: 単語から単語IDへのディクショナリ\n",
    "    :param id_to_word: 単語IDから単語へのディクショナリ\n",
    "    :param word_matrix: 単語ベクトルをまとめた行列。各行に対応する単語のベクトルが格納されていることを想定する\n",
    "    :param top: 上位何位まで表示するか\n",
    "    '''\n",
    "    if query not in word_to_id:\n",
    "        print('%s is not found' % query)\n",
    "        return\n",
    "\n",
    "    print('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "\n",
    "    vocab_size = len(id_to_word)\n",
    "\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return\n",
    "            \n",
    "# optimizer\n",
    "class Adam:\n",
    "    '''\n",
    "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
    "    '''\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "            \n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)\n",
    "\n",
    "# レイヤ\n",
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx\n",
    "\n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.out = softmax(x)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = self.out * dout\n",
    "        sumdx = np.sum(dx, axis=1, keepdims=True)\n",
    "        dx -= self.out * sumdx\n",
    "        return dx\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.y = None  # softmaxの出力\n",
    "        self.t = None  # 教師ラベル\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "\n",
    "        # 教師ラベルがone-hotベクトルの場合、正解のインデックスに変換\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "\n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = self.y.copy()\n",
    "        dx[np.arange(batch_size), self.t] -= 1\n",
    "        dx *= dout\n",
    "        dx = dx / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "# trainer\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_list = []\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=200):\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            # シャッフル\n",
    "            idx = np.random.permutation(np.arange(data_size))\n",
    "            x = x[idx]\n",
    "            t = t[idx]\n",
    "\n",
    "            for iters in range(max_iters):\n",
    "                batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "\n",
    "                # 勾配を求め、パラメータを更新\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicate(model.params, model.grads)  # 共有された重みを1つに集約\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # 評価\n",
    "                if (eval_interval is not None) and ((iters % eval_interval) == 0):\n",
    "                    avg_loss = total_loss / loss_count\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    if ((self.current_epoch % eval_interval) == 0)and((iters % 2) == 0):\n",
    "                        print('| epoch %d |  iter %d / %d | time %d[s] | loss %.2f'\n",
    "                              % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
    "                    self.loss_list.append(float(avg_loss))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = np.arange(len(self.loss_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.loss_list, label='train')\n",
    "        plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
