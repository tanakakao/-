{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-filing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "suited-consumption",
   "metadata": {},
   "source": [
    "## 8. 転移学習とファインチューニング\n",
    "学習済みの良いパラメータを利用したニューラルネットワークの学習に転移学習やファインチューニングがある。  \n",
    "出力層のみ学習させる手法を**転移学習**、出力層以外の畳み込み層なども学習させる手法を**ファインチューニング**と呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-scientist",
   "metadata": {},
   "source": [
    "#### keras\n",
    "#### 転移学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "continuing-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, GlobalAveragePooling2D,Input, Flatten, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "#from keras.applications.resnet_v2 import ResNet101V2\n",
    "#from keras.applications.inception_v3 import InceptionV3\n",
    "#from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "#from keras.applications.xception import Xception\n",
    "#from keras.applications.nasnet import NASNetLarge\n",
    "#from keras.applications.densenet import DenseNet121\n",
    "#from keras.applications.densenet import DenseNet201\n",
    "#from keras.applications.mobilenet_v2 import MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "photographic-anime",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"C:/Users/tanak/study/参考書/pytorchによる発展ディープラーニング/pytorch_advanced-master/1_image_classification/data/hymenoptera_data/train/\"\n",
    "val_dir = \"C:/Users/tanak/study/参考書/pytorchによる発展ディープラーニング/pytorch_advanced-master/1_image_classification/data/hymenoptera_data/val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "republican-peeing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ants\n",
      "bees\n",
      "ants\n",
      "bees\n"
     ]
    }
   ],
   "source": [
    "classes = os.listdir(train_dir)\n",
    "image_size = 224\n",
    "\n",
    "def preprocess(dir):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for idx, cls in enumerate(classes):\n",
    "        print(cls)\n",
    "        file_lists = glob.glob(os.path.join(dir, classes[idx]) + \"/*.jpg\")\n",
    "        for file in file_lists:\n",
    "            image = Image.open(file)\n",
    "            image = image.convert(\"RGB\")    \n",
    "            image = image.resize((image_size, image_size))\n",
    "            data = np.asarray(image)\n",
    "            x.append(data)\n",
    "            y.append(idx)\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    x = x.astype('float32')\n",
    "    x /= 255\n",
    "\n",
    "    y = keras.utils.to_categorical(y, num_classes)\n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = preprocess(train_dir)\n",
    "x_val, y_val = preprocess(val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "patent-wrapping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_classes  = 2\n",
    "size = 224\n",
    "batch_size = 32\n",
    "\n",
    "input_tensor = Input(shape=(size, size, 3))\n",
    "base_model = VGG16(weights='imagenet', include_top=False,input_tensor=input_tensor)\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "opposed-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "last = base_model.output\n",
    "x = Flatten()(last)\n",
    "x = Dense(4096, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "prediction = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "patent-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "jewish-acoustic",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(learning_rate=0.001, momentum=0.9, decay=5e-4)\n",
    "loss = \"binary_crossentropy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "external-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=loss,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "surface-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 3\n",
    "\n",
    "earlystop = EarlyStopping(patience=patience)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', # tensorflow 2.x\n",
    "                                            verbose=1, \n",
    "                                            factor=1/num_classes, # 1/ class数\n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "modelCheckpoint = ModelCheckpoint(filepath = \".\",\n",
    "                                  monitor='val_accuracy',\n",
    "                                  verbose=1,\n",
    "                                  save_best_only=True,\n",
    "                                  save_weights_only=False, # 重みのみ保存\n",
    "                                  mode='max', # val_accuracyの場合\n",
    "                                  save_freq=1)\n",
    "\n",
    "callbacks = [earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "addressed-transport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "8/8 [==============================] - 95s 12s/step - loss: 1.6796 - accuracy: 0.4650 - val_loss: 1.4208 - val_accuracy: 0.5425\n",
      "Epoch 2/15\n",
      "8/8 [==============================] - 97s 12s/step - loss: 1.2767 - accuracy: 0.5597 - val_loss: 0.4806 - val_accuracy: 0.8039\n",
      "Epoch 3/15\n",
      "8/8 [==============================] - 97s 12s/step - loss: 0.5544 - accuracy: 0.7613 - val_loss: 0.9571 - val_accuracy: 0.6209\n",
      "Epoch 4/15\n",
      "8/8 [==============================] - 98s 12s/step - loss: 0.5014 - accuracy: 0.7942 - val_loss: 0.5221 - val_accuracy: 0.7516\n",
      "Epoch 5/15\n",
      "8/8 [==============================] - 96s 12s/step - loss: 0.3563 - accuracy: 0.8560 - val_loss: 0.4264 - val_accuracy: 0.8301\n",
      "Epoch 6/15\n",
      "8/8 [==============================] - 96s 12s/step - loss: 0.2925 - accuracy: 0.8848 - val_loss: 0.7429 - val_accuracy: 0.6797\n",
      "Epoch 7/15\n",
      "8/8 [==============================] - 98s 12s/step - loss: 0.2573 - accuracy: 0.8765 - val_loss: 0.3781 - val_accuracy: 0.8562\n",
      "Epoch 8/15\n",
      "8/8 [==============================] - 96s 12s/step - loss: 0.2182 - accuracy: 0.9342 - val_loss: 0.3974 - val_accuracy: 0.8431\n",
      "Epoch 9/15\n",
      "8/8 [==============================] - 97s 12s/step - loss: 0.2064 - accuracy: 0.9259 - val_loss: 0.3720 - val_accuracy: 0.8627\n",
      "Epoch 10/15\n",
      "8/8 [==============================] - 96s 12s/step - loss: 0.1645 - accuracy: 0.9383 - val_loss: 0.3750 - val_accuracy: 0.8497\n",
      "Epoch 11/15\n",
      "8/8 [==============================] - 98s 12s/step - loss: 0.1575 - accuracy: 0.9300 - val_loss: 0.3743 - val_accuracy: 0.8562\n",
      "Epoch 12/15\n",
      "8/8 [==============================] - 100s 12s/step - loss: 0.2053 - accuracy: 0.9177 - val_loss: 0.3746 - val_accuracy: 0.8627\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "history = model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        initial_epoch = 0,\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_val, y_val),\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "literary-gilbert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: keras_teni_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('keras_teni_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-formation",
   "metadata": {},
   "source": [
    "#### ファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "assigned-mississippi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_classes  = 2\n",
    "size = 224\n",
    "batch_size = 32\n",
    "\n",
    "input_tensor = Input(shape=(size, size, 3))\n",
    "base_model = VGG16(weights='imagenet', include_top=False,input_tensor=input_tensor)\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "educational-biography",
   "metadata": {},
   "outputs": [],
   "source": [
    "last = base_model.output\n",
    "x = Flatten()(last)\n",
    "x = Dense(4096, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "prediction = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "settled-source",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in base_model.layers[15:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "fiscal-research",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(learning_rate=0.001, momentum=0.9, decay=5e-4)\n",
    "loss = \"binary_crossentropy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "statewide-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=loss,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "laden-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 3\n",
    "\n",
    "earlystop = EarlyStopping(patience=patience)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', # tensorflow 2.x\n",
    "                                            verbose=1, \n",
    "                                            factor=1/num_classes, # 1/ class数\n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "modelCheckpoint = ModelCheckpoint(filepath = \".\",\n",
    "                                  monitor='val_accuracy',\n",
    "                                  verbose=1,\n",
    "                                  save_best_only=True,\n",
    "                                  save_weights_only=False, # 重みのみ保存\n",
    "                                  mode='max', # val_accuracyの場合\n",
    "                                  save_freq=1)\n",
    "\n",
    "callbacks = [earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "tamil-correlation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "8/8 [==============================] - 81s 10s/step - loss: 0.8590 - accuracy: 0.5391 - val_loss: 0.5313 - val_accuracy: 0.7320\n",
      "Epoch 2/15\n",
      "8/8 [==============================] - 109s 14s/step - loss: 0.4818 - accuracy: 0.7572 - val_loss: 0.4873 - val_accuracy: 0.7712\n",
      "Epoch 3/15\n",
      "8/8 [==============================] - 114s 14s/step - loss: 0.2633 - accuracy: 0.8889 - val_loss: 0.4003 - val_accuracy: 0.8431\n",
      "Epoch 4/15\n",
      "8/8 [==============================] - 108s 13s/step - loss: 0.1840 - accuracy: 0.9095 - val_loss: 0.3781 - val_accuracy: 0.8693\n",
      "Epoch 5/15\n",
      "8/8 [==============================] - 109s 14s/step - loss: 0.1282 - accuracy: 0.9506 - val_loss: 0.4282 - val_accuracy: 0.8497\n",
      "Epoch 6/15\n",
      "8/8 [==============================] - 108s 13s/step - loss: 0.0913 - accuracy: 0.9671 - val_loss: 0.4753 - val_accuracy: 0.8627\n",
      "Epoch 7/15\n",
      "8/8 [==============================] - 107s 13s/step - loss: 0.0517 - accuracy: 0.9877 - val_loss: 0.4873 - val_accuracy: 0.8758\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "history = model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        initial_epoch = 0,\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_val, y_val),\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "cellular-webster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: keras_finetuning_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('keras_finetuning_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-legend",
   "metadata": {},
   "source": [
    "#### ptorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "checked-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizers\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "entertaining-teddy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to -\\.torch\\mnist\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fefe35eae99c4c389b6eb043548e9f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting -\\.torch\\mnist\\cifar-10-python.tar.gz to -\\.torch\\mnist\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "root = os.path.join('-', '.torch', 'mnist')\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Resize((128, 128))])\n",
    "mnist_train = datasets.CIFAR10(root=root,\n",
    "                             download=True,\n",
    "                             train=True, # 訓練データの取得\n",
    "                             transform=transform)\n",
    "mnist_test = datasets.CIFAR10(root=root,\n",
    "                            download=True,\n",
    "                            train=False,\n",
    "                            transform=transform)\n",
    "    \n",
    "n_samples = len(mnist_train)\n",
    "n_train = int(n_samples * 0.8)\n",
    "n_val = n_samples - n_train\n",
    "    \n",
    "mnist_train, mnist_val = random_split(mnist_train, [n_train, n_val])\n",
    "    \n",
    "train_dataloader = DataLoader(mnist_train,\n",
    "                              batch_size=100,\n",
    "                              shuffle=True) # 各エポックでデータのシャッフルを行う\n",
    "val_dataloader = DataLoader(mnist_val,\n",
    "                            batch_size=100,\n",
    "                            shuffle=False)\n",
    "test_dataloader = DataLoader(mnist_test,\n",
    "                             batch_size=100,\n",
    "                             shuffle=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "adjustable-extraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanak\\study\\参考書\\pytorchによる発展ディープラーニング\\pytorch_advanced-master\\1_image_classification\n",
      "./data/hymenoptera_data/train/**/*.jpg\n",
      "./data/hymenoptera_data/val/**/*.jpg\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\tanak\\study\\参考書\\pytorchによる発展ディープラーニング\\pytorch_advanced-master\\1_image_classification\n",
    "\n",
    "class ImageTransform():\n",
    "    def __init__(self, resize, mean, std):\n",
    "        self.data_transform = {\n",
    "            'train': transforms.Compose([\n",
    "                transforms.RandomResizedCrop(\n",
    "                    resize ,scale=(0.5, 1.0)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ]),\n",
    "            'val': transforms.Compose([\n",
    "                transforms.Resize(resize),\n",
    "                transforms.CenterCrop(resize),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ])\n",
    "        }\n",
    "    \n",
    "    def __call__(self, img, phase='train'):\n",
    "        return self.data_transform[phase](img)\n",
    "    \n",
    "def make_datapath_list(phase='train'):\n",
    "    root_path = \"./data/hymenoptera_data/\"\n",
    "    target_path = os.path.join(root_path+phase+'/**/*.jpg')\n",
    "    print(target_path)\n",
    "    \n",
    "    path_list = []\n",
    "    for path in glob.glob(target_path):\n",
    "        path_list.append(path)\n",
    "    \n",
    "    return path_list\n",
    "\n",
    "train_list = make_datapath_list(phase='train')\n",
    "val_list = make_datapath_list(phase='val')\n",
    "\n",
    "\n",
    "class HymenopteraDataset(data.Dataset):\n",
    "    def __init__(self, file_list, transform=None, phase='train'):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        self.phase = phase\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.file_list[index]\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        img_transformed = self.transform(img, self.phase)\n",
    "        \n",
    "        if self.phase == 'train':\n",
    "            label = img_path[30:34]\n",
    "        elif self.phase == 'val':\n",
    "            label = img_path[30:34]\n",
    "        \n",
    "        if label == 'ants':\n",
    "            label = 0\n",
    "        elif label == 'bees':\n",
    "            label = 1\n",
    "\n",
    "        return img_transformed, label\n",
    "\n",
    "size = 224\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_dataset = HymenopteraDataset(file_list=train_list, transform=ImageTransform(size, mean, std), phase='train')\n",
    "val_dataset = HymenopteraDataset(file_list=train_list, transform=ImageTransform(size, mean, std), phase='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "paperback-husband",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "expected-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
      "        0, 1, 1, 1, 1, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "batch_iterator = iter(val_dataloader)\n",
    "inputs, labels = next(batch_iterator)\n",
    "print(inputs.size())\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-unemployment",
   "metadata": {},
   "source": [
    "#### 転移学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "pregnant-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの読み込み\n",
    "#net = models.resnet18(pretrained=True)\n",
    "#net = models.alexnet(pretrained=True)\n",
    "net = models.vgg16(pretrained=True)\n",
    "#net = models.squeezenet1_0(pretrained=True)\n",
    "#net = models.densenet161(pretrained=True)\n",
    "#net = models.inception_v3(pretrained=True)\n",
    "#net = models.googlenet(pretrained=True)\n",
    "#net = models.shufflenet_v2_x1_0(pretrained=True)\n",
    "#net = models.mobilenet_v2(pretrained=True)\n",
    "#net = models.mobilenet_v3_large(pretrained=True)\n",
    "#net = models.mobilenet_v3_small(pretrained=True)\n",
    "#net = models.resnext50_32x4d(pretrained=True)\n",
    "#net = models.wide_resnet50_2(pretrained=True)\n",
    "#net = models.mnasnet1_0(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "convinced-crowd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=4096, out_features=1000, bias=True)\n",
      "Linear(in_features=4096, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# 勾配を計算させない\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "net.to(device)\n",
    "\n",
    "# 全結合層の変更・この層のみを学習させる\n",
    "num_frts = net.classifier[6].in_features\n",
    "print(net.classifier[6])\n",
    "net.classifier[6] = nn.Linear(in_features=num_frts, out_features=2)\n",
    "print(net.classifier[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "decreased-conviction",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optimizers.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "thrown-setting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 0.352, acc: 0.888, val_loss: 0.249, val_acc: 0.943\n",
      "epoch: 2, loss: 0.089, acc: 0.977, val_loss: 0.130, val_acc: 0.965\n"
     ]
    }
   ],
   "source": [
    "class EarlyStopping:\n",
    "    '''\n",
    "    早期終了 (early stopping)\n",
    "    定義する必要がある\n",
    "    '''\n",
    "    def __init__(self, patience=0, verbose=0):\n",
    "        self._step = 0\n",
    "        self._loss = float('inf')\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, loss):\n",
    "        if self._loss < loss:\n",
    "            self._step += 1\n",
    "            if self._step > self.patience:\n",
    "                if self.verbose:\n",
    "                    print('early stopping')\n",
    "                return True\n",
    "        else:\n",
    "            self._step = 0\n",
    "            self._loss = loss\n",
    "\n",
    "        return False\n",
    "\n",
    "def compute_loss(t, y):\n",
    "    return criterion(y, t)\n",
    "    \n",
    "def train_step(x, t):\n",
    "    net.train()\n",
    "    preds = net(x)\n",
    "    loss = compute_loss(t, preds)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, preds\n",
    "    \n",
    "def val_step(x, t):\n",
    "    net.eval()\n",
    "    preds = net(x)\n",
    "    loss = criterion(preds, t)\n",
    "    return loss, preds\n",
    "    \n",
    "epochs = 2\n",
    "hist = {'loss': [], 'accuracy': [],\n",
    "        'val_loss': [], 'val_accuracy': []}\n",
    "es = EarlyStopping(patience=2, verbose=1)\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    val_loss = 0.\n",
    "    val_acc = 0.\n",
    "        \n",
    "    for (x, t) in train_dataloader:\n",
    "        x, t = x.to(device), t.to(device)\n",
    "        loss, preds = train_step(x, t)\n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy_score(t.tolist(), preds.argmax(dim=-1).tolist())\n",
    "        \n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc /= len(train_dataloader)\n",
    "        \n",
    "    for (x, t) in val_dataloader:\n",
    "        x, t = x.to(device), t.to(device)\n",
    "        loss, preds = train_step(x, t)\n",
    "        val_loss += loss.item()\n",
    "        val_acc += accuracy_score(t.tolist(), preds.argmax(dim=-1).tolist())\n",
    "            \n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_acc /= len(val_dataloader)\n",
    "        \n",
    "    hist['loss'].append(train_loss)\n",
    "    hist['accuracy'].append(train_acc)\n",
    "    hist['val_loss'].append(val_loss)\n",
    "    hist['val_accuracy'].append(val_acc)\n",
    "        \n",
    "    print('epoch: {}, loss: {:.3f}, acc: {:.3f}, val_loss: {:.3f}, val_acc: {:.3f}'.format(epoch+1, train_loss, train_acc, val_loss, val_acc))\n",
    "        \n",
    "    if es(val_loss): # 早期終了判定\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "blind-shipping",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.to('cpu').state_dict(), 'cnn_teni_model_pytorch.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-blogger",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "surprising-queens",
   "metadata": {},
   "source": [
    "#### ファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "cultural-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの読み込み\n",
    "#net = models.resnet18(pretrained=True)\n",
    "#net = models.alexnet(pretrained=True)\n",
    "net = models.vgg16(pretrained=True)\n",
    "#net = models.squeezenet1_0(pretrained=True)\n",
    "#net = models.densenet161(pretrained=True)\n",
    "#net = models.inception_v3(pretrained=True)\n",
    "#net = models.googlenet(pretrained=True)\n",
    "#net = models.shufflenet_v2_x1_0(pretrained=True)\n",
    "#net = models.mobilenet_v2(pretrained=True)\n",
    "#net = models.mobilenet_v3_large(pretrained=True)\n",
    "#net = models.mobilenet_v3_small(pretrained=True)\n",
    "#net = models.resnext50_32x4d(pretrained=True)\n",
    "#net = models.wide_resnet50_2(pretrained=True)\n",
    "#net = models.mnasnet1_0(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "fresh-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "respected-trade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           1,792\n",
      "              ReLU-2         [-1, 64, 256, 256]               0\n",
      "            Conv2d-3         [-1, 64, 256, 256]          36,928\n",
      "              ReLU-4         [-1, 64, 256, 256]               0\n",
      "         MaxPool2d-5         [-1, 64, 128, 128]               0\n",
      "            Conv2d-6        [-1, 128, 128, 128]          73,856\n",
      "              ReLU-7        [-1, 128, 128, 128]               0\n",
      "            Conv2d-8        [-1, 128, 128, 128]         147,584\n",
      "              ReLU-9        [-1, 128, 128, 128]               0\n",
      "        MaxPool2d-10          [-1, 128, 64, 64]               0\n",
      "           Conv2d-11          [-1, 256, 64, 64]         295,168\n",
      "             ReLU-12          [-1, 256, 64, 64]               0\n",
      "           Conv2d-13          [-1, 256, 64, 64]         590,080\n",
      "             ReLU-14          [-1, 256, 64, 64]               0\n",
      "           Conv2d-15          [-1, 256, 64, 64]         590,080\n",
      "             ReLU-16          [-1, 256, 64, 64]               0\n",
      "        MaxPool2d-17          [-1, 256, 32, 32]               0\n",
      "           Conv2d-18          [-1, 512, 32, 32]       1,180,160\n",
      "             ReLU-19          [-1, 512, 32, 32]               0\n",
      "           Conv2d-20          [-1, 512, 32, 32]       2,359,808\n",
      "             ReLU-21          [-1, 512, 32, 32]               0\n",
      "           Conv2d-22          [-1, 512, 32, 32]       2,359,808\n",
      "             ReLU-23          [-1, 512, 32, 32]               0\n",
      "        MaxPool2d-24          [-1, 512, 16, 16]               0\n",
      "           Conv2d-25          [-1, 512, 16, 16]       2,359,808\n",
      "             ReLU-26          [-1, 512, 16, 16]               0\n",
      "           Conv2d-27          [-1, 512, 16, 16]       2,359,808\n",
      "             ReLU-28          [-1, 512, 16, 16]               0\n",
      "           Conv2d-29          [-1, 512, 16, 16]       2,359,808\n",
      "             ReLU-30          [-1, 512, 16, 16]               0\n",
      "        MaxPool2d-31            [-1, 512, 8, 8]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "             ReLU-34                 [-1, 4096]               0\n",
      "          Dropout-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 4096]      16,781,312\n",
      "             ReLU-37                 [-1, 4096]               0\n",
      "          Dropout-38                 [-1, 4096]               0\n",
      "           Linear-39                 [-1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 285.64\n",
      "Params size (MB): 527.79\n",
      "Estimated Total Size (MB): 814.18\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "print(summary(net, (3, 256, 256)))\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "stylish-christianity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=4096, out_features=1000, bias=True)\n",
      "Linear(in_features=4096, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# 全結合層の変更\n",
    "print(net.classifier[6])\n",
    "net.classifier[6] = nn.Linear(in_features=4096, out_features=2)\n",
    "print(net.classifier[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "motivated-grain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name :  features.0.weight\n",
      "name :  features.0.bias\n",
      "name :  features.2.weight\n",
      "name :  features.2.bias\n",
      "name :  features.5.weight\n",
      "name :  features.5.bias\n",
      "name :  features.7.weight\n",
      "name :  features.7.bias\n",
      "name :  features.10.weight\n",
      "name :  features.10.bias\n",
      "name :  features.12.weight\n",
      "name :  features.12.bias\n",
      "name :  features.14.weight\n",
      "name :  features.14.bias\n",
      "name :  features.17.weight\n",
      "name :  features.17.bias\n",
      "name :  features.19.weight\n",
      "name :  features.19.bias\n",
      "name :  features.21.weight\n",
      "name :  features.21.bias\n",
      "name :  features.24.weight\n",
      "name :  features.24.bias\n",
      "name :  features.26.weight\n",
      "name :  features.26.bias\n",
      "name :  features.28.weight\n",
      "name :  features.28.bias\n",
      "name :  classifier.0.weight\n",
      "name :  classifier.0.bias\n",
      "name :  classifier.3.weight\n",
      "name :  classifier.3.bias\n",
      "name :  classifier.6.weight\n",
      "name :  classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print('name : ', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "physical-buying",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params_to_update_1に格納： features.0.weight\n",
      "params_to_update_1に格納： features.0.bias\n",
      "params_to_update_1に格納： features.2.weight\n",
      "params_to_update_1に格納： features.2.bias\n",
      "params_to_update_1に格納： features.5.weight\n",
      "params_to_update_1に格納： features.5.bias\n",
      "params_to_update_1に格納： features.7.weight\n",
      "params_to_update_1に格納： features.7.bias\n",
      "params_to_update_1に格納： features.10.weight\n",
      "params_to_update_1に格納： features.10.bias\n",
      "params_to_update_1に格納： features.12.weight\n",
      "params_to_update_1に格納： features.12.bias\n",
      "params_to_update_1に格納： features.14.weight\n",
      "params_to_update_1に格納： features.14.bias\n",
      "params_to_update_1に格納： features.17.weight\n",
      "params_to_update_1に格納： features.17.bias\n",
      "params_to_update_1に格納： features.19.weight\n",
      "params_to_update_1に格納： features.19.bias\n",
      "params_to_update_1に格納： features.21.weight\n",
      "params_to_update_1に格納： features.21.bias\n",
      "params_to_update_1に格納： features.24.weight\n",
      "params_to_update_1に格納： features.24.bias\n",
      "params_to_update_1に格納： features.26.weight\n",
      "params_to_update_1に格納： features.26.bias\n",
      "params_to_update_1に格納： features.28.weight\n",
      "params_to_update_1に格納： features.28.bias\n",
      "params_to_update_2に格納： classifier.0.weight\n",
      "params_to_update_2に格納： classifier.0.bias\n",
      "params_to_update_2に格納： classifier.3.weight\n",
      "params_to_update_2に格納： classifier.3.bias\n",
      "params_to_update_3に格納： classifier.6.weight\n",
      "params_to_update_3に格納： classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "# featureモジュール\n",
    "params_to_update_1 = []\n",
    "# classifierモジュール(後半)\n",
    "params_to_update_2 = []\n",
    "# classifierモジュール(付け替えた層)\n",
    "params_to_update_3 = []\n",
    "\n",
    "# 学習させる層のパラメータ名を指定\n",
    "update_param_names_1 = ['features']\n",
    "update_param_names_2 = ['classifier.0.weight', 'classifier.0.bias',\n",
    "                        'classifier.3.weight', 'classifier.3.bias']\n",
    "update_param_names_3 = ['classifier.6.weight', 'classifier.6.bias']\n",
    "\n",
    "# パラメータごとに各リストに格納\n",
    "for name, param in net.named_parameters():\n",
    "\n",
    "    if update_param_names_1[0] in name:\n",
    "        param.requires_grad = True\n",
    "        params_to_update_1.append(param)\n",
    "        print(\"params_to_update_1に格納：\", name)\n",
    "    \n",
    "    elif name in update_param_names_2:\n",
    "        param.requires_grad = True\n",
    "        params_to_update_2.append(param)\n",
    "        print(\"params_to_update_2に格納：\", name)\n",
    "    \n",
    "    elif name in update_param_names_3:\n",
    "        param.requires_grad = True\n",
    "        params_to_update_3.append(param)\n",
    "        print(\"params_to_update_3に格納：\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "champion-decline",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.SGD([\n",
    "    {'params': params_to_update_1, 'lr': 1e-4},\n",
    "    {'params': params_to_update_2, 'lr': 5e-4},\n",
    "    {'params': params_to_update_3, 'lr': 1e-3},\n",
    "], momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "active-apparel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 0.494, acc: 0.771, val_loss: 0.151, val_acc: 0.939\n",
      "epoch: 2, loss: 0.067, acc: 0.977, val_loss: 0.069, val_acc: 0.979\n"
     ]
    }
   ],
   "source": [
    "class EarlyStopping:\n",
    "    '''\n",
    "    早期終了 (early stopping)\n",
    "    定義する必要がある\n",
    "    '''\n",
    "    def __init__(self, patience=0, verbose=0):\n",
    "        self._step = 0\n",
    "        self._loss = float('inf')\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, loss):\n",
    "        if self._loss < loss:\n",
    "            self._step += 1\n",
    "            if self._step > self.patience:\n",
    "                if self.verbose:\n",
    "                    print('early stopping')\n",
    "                return True\n",
    "        else:\n",
    "            self._step = 0\n",
    "            self._loss = loss\n",
    "\n",
    "        return False\n",
    "\n",
    "def compute_loss(t, y):\n",
    "    return criterion(y, t)\n",
    "    \n",
    "def train_step(x, t):\n",
    "    net.train()\n",
    "    preds = net(x)\n",
    "    loss = compute_loss(t, preds)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, preds\n",
    "    \n",
    "def val_step(x, t):\n",
    "    net.eval()\n",
    "    preds = net(x)\n",
    "    loss = criterion(preds, t)\n",
    "    return loss, preds\n",
    "    \n",
    "epochs = 2\n",
    "hist = {'loss': [], 'accuracy': [],\n",
    "        'val_loss': [], 'val_accuracy': []}\n",
    "es = EarlyStopping(patience=2, verbose=1)\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    val_loss = 0.\n",
    "    val_acc = 0.\n",
    "        \n",
    "    for (x, t) in train_dataloader:\n",
    "        x, t = x.to(device), t.to(device)\n",
    "        loss, preds = train_step(x, t)\n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy_score(t.tolist(), preds.argmax(dim=-1).tolist())\n",
    "        \n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc /= len(train_dataloader)\n",
    "        \n",
    "    for (x, t) in val_dataloader:\n",
    "        x, t = x.to(device), t.to(device)\n",
    "        loss, preds = train_step(x, t)\n",
    "        val_loss += loss.item()\n",
    "        val_acc += accuracy_score(t.tolist(), preds.argmax(dim=-1).tolist())\n",
    "            \n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_acc /= len(val_dataloader)\n",
    "        \n",
    "    hist['loss'].append(train_loss)\n",
    "    hist['accuracy'].append(train_acc)\n",
    "    hist['val_loss'].append(val_loss)\n",
    "    hist['val_accuracy'].append(val_acc)\n",
    "        \n",
    "    print('epoch: {}, loss: {:.3f}, acc: {:.3f}, val_loss: {:.3f}, val_acc: {:.3f}'.format(epoch+1, train_loss, train_acc, val_loss, val_acc))\n",
    "        \n",
    "    if es(val_loss): # 早期終了判定\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "designing-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.to('cpu').state_dict(), 'cnn_finetuning_model_pytorch.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-cattle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
