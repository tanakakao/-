{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "charitable-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Activation, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization, Add\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "protecting-mandate",
   "metadata": {},
   "outputs": [],
   "source": [
    "class wide_res_block(Model):\n",
    "    \"\"\"wide residual block\"\"\"\n",
    "    def __init__(self, out_channels, block_num, layer_num):\n",
    "        super(wide_res_block, self).__init__(name='block'+'_'+str(block_num)+'_'+str(layer_num))        \n",
    "        block_name = '_'+str(block_num)+'_'+str(layer_num)\n",
    "        \n",
    "        # shortcutとstrideの設定\n",
    "        if (layer_num == 0):\n",
    "            # 最初のresblockは(W､ H)は変更しないのでstrideは1にする\n",
    "            if (block_num==1):\n",
    "                self._is_change = False\n",
    "                stride = 1\n",
    "            else:\n",
    "                self._is_change = True\n",
    "                stride = 2\n",
    "            self.conv_sc = Conv2D(out_channels, kernel_size=1, strides=stride, padding='same', use_bias=False, name='conv_sc'+block_name)\n",
    "            self.bn_sc = BatchNormalization(name='bn_sc'+block_name)\n",
    "        else:\n",
    "            self._is_change = False\n",
    "            stride = 1\n",
    "\n",
    "        # 1層目 3×3 畳み込み処理を行います\n",
    "        self.bn1 = BatchNormalization(name='bn1'+block_name)\n",
    "        self.act1 = Activation('relu', name='act1'+block_name)\n",
    "        self.drop1 = Dropout(rate=0.3, name='drop1'+block_name)\n",
    "        self.conv1 = Conv2D(out_channels, kernel_size=3, strides=stride, padding='same', use_bias=False, name='conv1'+block_name)\n",
    "        \n",
    "        # 2層目 3×3 畳み込み処理を行います\n",
    "        self.bn2 = BatchNormalization(name='bn2'+block_name)\n",
    "        self.act2 = Activation('relu', name='act2'+block_name)\n",
    "        self.drop2 = Dropout(rate=0.3, name='drop2'+block_name)\n",
    "        self.conv2 = Conv2D(out_channels, kernel_size=3, strides=1, padding='same', use_bias=False, name='conv2'+block_name)\n",
    "        \n",
    "        self.add = Add(name='add'+block_name)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = self.act1(out)\n",
    "        out = self.drop1(out)\n",
    "        out = self.conv1(out)\n",
    "        \n",
    "        out = self.bn2(out)\n",
    "        out = self.act2(out)\n",
    "        out = self.drop2(out)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        if K.int_shape(x) != K.int_shape(out):\n",
    "            shortcut = self.conv_sc(x)\n",
    "            shortcut = self.bn_sc(shortcut)\n",
    "        else:\n",
    "            shortcut = x\n",
    "        \n",
    "        out = self.add([out, shortcut])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "otherwise-census",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_21\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, 224, 224, 64 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bn1_1_0 (BatchNormalization)    (None, 224, 224, 64) 256         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "act1_1_0 (Activation)           (None, 224, 224, 64) 0           bn1_1_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "drop1_1_0 (Dropout)             (None, 224, 224, 64) 0           act1_1_0[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1_1_0 (Conv2D)              (None, 224, 224, 128 73728       drop1_1_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn2_1_0 (BatchNormalization)    (None, 224, 224, 128 512         conv1_1_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "act2_1_0 (Activation)           (None, 224, 224, 128 0           bn2_1_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "drop2_1_0 (Dropout)             (None, 224, 224, 128 0           act2_1_0[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_sc_1_0 (Conv2D)            (None, 224, 224, 128 8192        input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_0 (Conv2D)              (None, 224, 224, 128 147456      drop2_1_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_sc_1_0 (BatchNormalization)  (None, 224, 224, 128 512         conv_sc_1_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_1_0 (Add)                   (None, 224, 224, 128 0           conv2_1_0[0][0]                  \n",
      "                                                                 bn_sc_1_0[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 230,656\n",
      "Trainable params: 230,016\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = wide_res_block(out_channels=128, block_num=1, layer_num=0)\n",
    "model.build((None, 224, 224, 64))  # build with input shape.\n",
    "dummy_input = Input(shape=(224, 224, 64))  # declare without batch demension.\n",
    "model_summary = Model(inputs=[dummy_input], outputs=model.call(dummy_input))\n",
    "model_summary.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "italic-bottle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideResNet(Model):\n",
    "    def __init__(self, depth, k, num_classes=10):\n",
    "        super().__init__()\n",
    "        self._layers = []\n",
    "        \n",
    "        # 各ネットワークでのブロックの数\n",
    "        N = (depth - 4) // 6\n",
    "        \n",
    "        # 入力層\n",
    "        self._layers += [\n",
    "            Conv2D(filters = 16, kernel_size = 3, strides = 1, padding = 'same', name='conv_input')\n",
    "        ]\n",
    "\n",
    "        # Residualブロック\n",
    "        self._layers += [wide_res_block(out_channels=16*k, block_num=1, layer_num=i) for i in range(N)]\n",
    "        self._layers += [wide_res_block(out_channels=32*k, block_num=2, layer_num=i) for i in range(N)]\n",
    "        self._layers += [wide_res_block(out_channels=64*k, block_num=3, layer_num=i) for i in range(N)]\n",
    "\n",
    "        # 出力層\n",
    "        self._layers += [\n",
    "            BatchNormalization(name='bn_input'),\n",
    "            Activation('relu', name='act_input'),\n",
    "            GlobalAveragePooling2D(name='pool_output'),\n",
    "            Dense(num_classes, activation='softmax', name='output')\n",
    "        ]\n",
    "    \n",
    "    def call(self, x):\n",
    "        for layer in self._layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "breathing-davis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv_input (Conv2D)          (None, 32, 32, 16)        448       \n",
      "_________________________________________________________________\n",
      "block_1_0 (wide_res_block)   (None, 32, 32, 160)       257344    \n",
      "_________________________________________________________________\n",
      "block_1_1 (wide_res_block)   (None, 32, 32, 160)       462080    \n",
      "_________________________________________________________________\n",
      "block_1_2 (wide_res_block)   (None, 32, 32, 160)       462080    \n",
      "_________________________________________________________________\n",
      "block_1_3 (wide_res_block)   (None, 32, 32, 160)       462080    \n",
      "_________________________________________________________________\n",
      "block_2_0 (wide_res_block)   (None, 16, 16, 320)       1436800   \n",
      "_________________________________________________________________\n",
      "block_2_1 (wide_res_block)   (None, 16, 16, 320)       1845760   \n",
      "_________________________________________________________________\n",
      "block_2_2 (wide_res_block)   (None, 16, 16, 320)       1845760   \n",
      "_________________________________________________________________\n",
      "block_2_3 (wide_res_block)   (None, 16, 16, 320)       1845760   \n",
      "_________________________________________________________________\n",
      "block_3_0 (wide_res_block)   (None, 8, 8, 640)         5740800   \n",
      "_________________________________________________________________\n",
      "block_3_1 (wide_res_block)   (None, 8, 8, 640)         7377920   \n",
      "_________________________________________________________________\n",
      "block_3_2 (wide_res_block)   (None, 8, 8, 640)         7377920   \n",
      "_________________________________________________________________\n",
      "block_3_3 (wide_res_block)   (None, 8, 8, 640)         7377920   \n",
      "_________________________________________________________________\n",
      "bn_input (BatchNormalization (None, 8, 8, 640)         2560      \n",
      "_________________________________________________________________\n",
      "act_input (Activation)       (None, 8, 8, 640)         0         \n",
      "_________________________________________________________________\n",
      "pool_output (GlobalAveragePo (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                6410      \n",
      "=================================================================\n",
      "Total params: 36,501,642\n",
      "Trainable params: 36,481,450\n",
      "Non-trainable params: 20,192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = WideResNet(depth=28, k=10, num_classes=10)\n",
    "model.build((None, 32, 32, 3))  # build with input shape.\n",
    "dummy_input = Input(shape=(32, 32, 3))  # declare without batch demension.\n",
    "model_summary = Model(inputs=[dummy_input], outputs=model.call(dummy_input))\n",
    "model_summary.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "private-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習率を返す関数を用意する\n",
    "def lr_schedul(epoch):\n",
    "    x = 0.1\n",
    "    if epoch >= 60:\n",
    "        x = 0.1*0.2\n",
    "    if epoch >= 120:\n",
    "        x = 0.1*(0.2**2)\n",
    "    if epoch >= 160:\n",
    "        x = 0.1*(0.2**3)\n",
    "    return x\n",
    "\n",
    "\n",
    "lr_decay = LearningRateScheduler(\n",
    "    lr_schedul,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "sgd = SGD(lr=0.1, momentum=0.9, decay=1e-4, nesterov=True)\n",
    "model.compile(loss=['categorical_crossentropy'], optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-reverse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "intelligent-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import Accuracy as accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "alien-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "class wide_res_block(nn.Module):\n",
    "    \"\"\"wide residual block\"\"\"\n",
    "    def __init__(self, out_channels, block_num, layer_num):\n",
    "        super(wide_res_block, self).__init__()\n",
    "        \n",
    "        # 1番目のブロック以外はチャンネル数がinputとoutputで変わる(output=4×input)\n",
    "        if (layer_num==0):\n",
    "            if (block_num==1):\n",
    "                input_channels = 16\n",
    "            else:\n",
    "                input_channels = out_channels//2\n",
    "        else:\n",
    "            input_channels = out_channels\n",
    "\n",
    "        # shortcutとstrideの設定\n",
    "        if (layer_num == 0):\n",
    "            self._is_change = True\n",
    "            # 最初のresblockは(W､ H)は変更しないのでstrideは1にする\n",
    "            if (block_num==1):\n",
    "                stride = 1\n",
    "            else:\n",
    "                stride = 2\n",
    "            \n",
    "            self.conv_sc = nn.Conv2d(input_channels, out_channels, kernel_size=1, stride=stride)\n",
    "            self.bn_sc = nn.BatchNorm2d(out_channels)\n",
    "        else:\n",
    "            self._is_change = False\n",
    "            stride = 1\n",
    "\n",
    "        # 1層目 3×3 畳み込み処理を行います\n",
    "        self.bn1 = nn.BatchNorm2d(input_channels)\n",
    "        self.drop1 = nn.Dropout(p=0.3)\n",
    "        self.conv1 = nn.Conv2d(input_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        \n",
    "        \n",
    "        # 2層目 3×3 畳み込み処理を行います\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.drop2 = nn.Dropout(p=0.3)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.drop1(out)\n",
    "        out = self.conv1(out)\n",
    "        \n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.drop2(out)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        # Projection shortcutの場合\n",
    "        if self._is_change:\n",
    "            shortcut = self.conv_sc(shortcut)\n",
    "            shortcut = self.bn_sc(shortcut)\n",
    "        \n",
    "        out += shortcut\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "thrown-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, depth, k, num_classes=10):\n",
    "        super(WideResNet, self).__init__()\n",
    "        \n",
    "        # 各ネットワークでのブロックの数\n",
    "        N = (depth - 4) // 6\n",
    "        \n",
    "        conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv1 = nn.Sequential(*[conv1])\n",
    "        \n",
    "        self.conv2_x = nn.Sequential(*[wide_res_block(out_channels=16*k, block_num=1, layer_num=i) for i in range(N)])\n",
    "        self.conv3_x = nn.Sequential(*[wide_res_block(out_channels=32*k, block_num=2, layer_num=i) for i in range(N)])\n",
    "        self.conv4_x = nn.Sequential(*[wide_res_block(out_channels=64*k, block_num=3, layer_num=i) for i in range(N)])\n",
    "        \n",
    "        bn = nn.BatchNorm2d(64*k)\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "        pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Sequential(*[bn, relu, pool])\n",
    "        \n",
    "        self.linear = nn.Linear(in_features=640, out_features=num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2_x(out)\n",
    "        out = self.conv3_x(out)\n",
    "        out = self.conv4_x(out)\n",
    "        out = self.fc(out)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "proper-alabama",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 32, 32]             448\n",
      "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
      "              ReLU-3           [-1, 16, 32, 32]               0\n",
      "           Dropout-4           [-1, 16, 32, 32]               0\n",
      "            Conv2d-5          [-1, 160, 32, 32]          23,200\n",
      "       BatchNorm2d-6          [-1, 160, 32, 32]             320\n",
      "              ReLU-7          [-1, 160, 32, 32]               0\n",
      "           Dropout-8          [-1, 160, 32, 32]               0\n",
      "            Conv2d-9          [-1, 160, 32, 32]         230,560\n",
      "           Conv2d-10          [-1, 160, 32, 32]           2,720\n",
      "      BatchNorm2d-11          [-1, 160, 32, 32]             320\n",
      "   wide_res_block-12          [-1, 160, 32, 32]               0\n",
      "      BatchNorm2d-13          [-1, 160, 32, 32]             320\n",
      "             ReLU-14          [-1, 160, 32, 32]               0\n",
      "          Dropout-15          [-1, 160, 32, 32]               0\n",
      "           Conv2d-16          [-1, 160, 32, 32]         230,560\n",
      "      BatchNorm2d-17          [-1, 160, 32, 32]             320\n",
      "             ReLU-18          [-1, 160, 32, 32]               0\n",
      "          Dropout-19          [-1, 160, 32, 32]               0\n",
      "           Conv2d-20          [-1, 160, 32, 32]         230,560\n",
      "   wide_res_block-21          [-1, 160, 32, 32]               0\n",
      "      BatchNorm2d-22          [-1, 160, 32, 32]             320\n",
      "             ReLU-23          [-1, 160, 32, 32]               0\n",
      "          Dropout-24          [-1, 160, 32, 32]               0\n",
      "           Conv2d-25          [-1, 160, 32, 32]         230,560\n",
      "      BatchNorm2d-26          [-1, 160, 32, 32]             320\n",
      "             ReLU-27          [-1, 160, 32, 32]               0\n",
      "          Dropout-28          [-1, 160, 32, 32]               0\n",
      "           Conv2d-29          [-1, 160, 32, 32]         230,560\n",
      "   wide_res_block-30          [-1, 160, 32, 32]               0\n",
      "      BatchNorm2d-31          [-1, 160, 32, 32]             320\n",
      "             ReLU-32          [-1, 160, 32, 32]               0\n",
      "          Dropout-33          [-1, 160, 32, 32]               0\n",
      "           Conv2d-34          [-1, 160, 32, 32]         230,560\n",
      "      BatchNorm2d-35          [-1, 160, 32, 32]             320\n",
      "             ReLU-36          [-1, 160, 32, 32]               0\n",
      "          Dropout-37          [-1, 160, 32, 32]               0\n",
      "           Conv2d-38          [-1, 160, 32, 32]         230,560\n",
      "   wide_res_block-39          [-1, 160, 32, 32]               0\n",
      "      BatchNorm2d-40          [-1, 160, 32, 32]             320\n",
      "             ReLU-41          [-1, 160, 32, 32]               0\n",
      "          Dropout-42          [-1, 160, 32, 32]               0\n",
      "           Conv2d-43          [-1, 320, 16, 16]         461,120\n",
      "      BatchNorm2d-44          [-1, 320, 16, 16]             640\n",
      "             ReLU-45          [-1, 320, 16, 16]               0\n",
      "          Dropout-46          [-1, 320, 16, 16]               0\n",
      "           Conv2d-47          [-1, 320, 16, 16]         921,920\n",
      "           Conv2d-48          [-1, 320, 16, 16]          51,520\n",
      "      BatchNorm2d-49          [-1, 320, 16, 16]             640\n",
      "   wide_res_block-50          [-1, 320, 16, 16]               0\n",
      "      BatchNorm2d-51          [-1, 320, 16, 16]             640\n",
      "             ReLU-52          [-1, 320, 16, 16]               0\n",
      "          Dropout-53          [-1, 320, 16, 16]               0\n",
      "           Conv2d-54          [-1, 320, 16, 16]         921,920\n",
      "      BatchNorm2d-55          [-1, 320, 16, 16]             640\n",
      "             ReLU-56          [-1, 320, 16, 16]               0\n",
      "          Dropout-57          [-1, 320, 16, 16]               0\n",
      "           Conv2d-58          [-1, 320, 16, 16]         921,920\n",
      "   wide_res_block-59          [-1, 320, 16, 16]               0\n",
      "      BatchNorm2d-60          [-1, 320, 16, 16]             640\n",
      "             ReLU-61          [-1, 320, 16, 16]               0\n",
      "          Dropout-62          [-1, 320, 16, 16]               0\n",
      "           Conv2d-63          [-1, 320, 16, 16]         921,920\n",
      "      BatchNorm2d-64          [-1, 320, 16, 16]             640\n",
      "             ReLU-65          [-1, 320, 16, 16]               0\n",
      "          Dropout-66          [-1, 320, 16, 16]               0\n",
      "           Conv2d-67          [-1, 320, 16, 16]         921,920\n",
      "   wide_res_block-68          [-1, 320, 16, 16]               0\n",
      "      BatchNorm2d-69          [-1, 320, 16, 16]             640\n",
      "             ReLU-70          [-1, 320, 16, 16]               0\n",
      "          Dropout-71          [-1, 320, 16, 16]               0\n",
      "           Conv2d-72          [-1, 320, 16, 16]         921,920\n",
      "      BatchNorm2d-73          [-1, 320, 16, 16]             640\n",
      "             ReLU-74          [-1, 320, 16, 16]               0\n",
      "          Dropout-75          [-1, 320, 16, 16]               0\n",
      "           Conv2d-76          [-1, 320, 16, 16]         921,920\n",
      "   wide_res_block-77          [-1, 320, 16, 16]               0\n",
      "      BatchNorm2d-78          [-1, 320, 16, 16]             640\n",
      "             ReLU-79          [-1, 320, 16, 16]               0\n",
      "          Dropout-80          [-1, 320, 16, 16]               0\n",
      "           Conv2d-81            [-1, 640, 8, 8]       1,843,840\n",
      "      BatchNorm2d-82            [-1, 640, 8, 8]           1,280\n",
      "             ReLU-83            [-1, 640, 8, 8]               0\n",
      "          Dropout-84            [-1, 640, 8, 8]               0\n",
      "           Conv2d-85            [-1, 640, 8, 8]       3,687,040\n",
      "           Conv2d-86            [-1, 640, 8, 8]         205,440\n",
      "      BatchNorm2d-87            [-1, 640, 8, 8]           1,280\n",
      "   wide_res_block-88            [-1, 640, 8, 8]               0\n",
      "      BatchNorm2d-89            [-1, 640, 8, 8]           1,280\n",
      "             ReLU-90            [-1, 640, 8, 8]               0\n",
      "          Dropout-91            [-1, 640, 8, 8]               0\n",
      "           Conv2d-92            [-1, 640, 8, 8]       3,687,040\n",
      "      BatchNorm2d-93            [-1, 640, 8, 8]           1,280\n",
      "             ReLU-94            [-1, 640, 8, 8]               0\n",
      "          Dropout-95            [-1, 640, 8, 8]               0\n",
      "           Conv2d-96            [-1, 640, 8, 8]       3,687,040\n",
      "   wide_res_block-97            [-1, 640, 8, 8]               0\n",
      "      BatchNorm2d-98            [-1, 640, 8, 8]           1,280\n",
      "             ReLU-99            [-1, 640, 8, 8]               0\n",
      "         Dropout-100            [-1, 640, 8, 8]               0\n",
      "          Conv2d-101            [-1, 640, 8, 8]       3,687,040\n",
      "     BatchNorm2d-102            [-1, 640, 8, 8]           1,280\n",
      "            ReLU-103            [-1, 640, 8, 8]               0\n",
      "         Dropout-104            [-1, 640, 8, 8]               0\n",
      "          Conv2d-105            [-1, 640, 8, 8]       3,687,040\n",
      "  wide_res_block-106            [-1, 640, 8, 8]               0\n",
      "     BatchNorm2d-107            [-1, 640, 8, 8]           1,280\n",
      "            ReLU-108            [-1, 640, 8, 8]               0\n",
      "         Dropout-109            [-1, 640, 8, 8]               0\n",
      "          Conv2d-110            [-1, 640, 8, 8]       3,687,040\n",
      "     BatchNorm2d-111            [-1, 640, 8, 8]           1,280\n",
      "            ReLU-112            [-1, 640, 8, 8]               0\n",
      "         Dropout-113            [-1, 640, 8, 8]               0\n",
      "          Conv2d-114            [-1, 640, 8, 8]       3,687,040\n",
      "  wide_res_block-115            [-1, 640, 8, 8]               0\n",
      "     BatchNorm2d-116            [-1, 640, 8, 8]           1,280\n",
      "            ReLU-117            [-1, 640, 8, 8]               0\n",
      "AdaptiveAvgPool2d-118            [-1, 640, 1, 1]               0\n",
      "          Linear-119                   [-1, 10]           6,410\n",
      "================================================================\n",
      "Total params: 36,491,530\n",
      "Trainable params: 36,491,530\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 83.32\n",
      "Params size (MB): 139.20\n",
      "Estimated Total Size (MB): 222.53\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(WideResNet(depth=28, k=10, num_classes=10), (3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "noted-forge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WRNTrainer(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = WideResNet(depth=28, k=10, num_classes=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch \n",
    "        #x, y = x.to(device), y.to(device)\n",
    "        y_hat = self.forward(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        return {'loss': loss, 'y_hat':y_hat, 'y':y, 'batch_loss': loss.item()*x.size(0)}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        #x, y = x.to(device), y.to(device)\n",
    "        y_hat = self.forward(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        return {'y_hat':y_hat, 'y':y, 'batch_loss': loss.item()*x.size(0)}\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        #x, y = x.to(device), y.to(device)\n",
    "        y_hat = self.forward(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        y_label = torch.argmax(y_hat, dim=1)\n",
    "        acc = accuracy()(y_label, y)\n",
    "        return {'test_loss': loss, 'test_acc': acc}\n",
    "    \n",
    "    def training_epoch_end(self, train_step_output):\n",
    "        y_hat = torch.cat([val['y_hat'] for val in train_step_outputs], dim=0)\n",
    "        y = torch.cat([val['y'] for val in train_step_outputs], dim=0)\n",
    "        epoch_loss = sum([val['batch_loss'] for val in train_step_outputs]) / y_hat.size(0)\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        acc = accuracy()(preds, y)\n",
    "        self.log('train_loss', epoch_loss, prog_bar=True, on_epoch=True)\n",
    "        self.log('train_acc', acc, prog_bar=True, on_epoch=True)\n",
    "        \n",
    "        print('---------- Current Epoch {} ----------'.format(self.current_epoch + 1))\n",
    "        print('train Loss: {:.4f} train Acc: {:.4f}'.format(epoch_loass, acc))\n",
    "    \n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        y_hat = torch.cat([val['y_hat'] for val in val_step_outputs], dim=0)\n",
    "        y = torch.cat([val['y'] for val in val_step_outputs], dim=0)\n",
    "        epoch_loss = sum([val['batch_loss'] for val in val_step_outputs]) / y_hat.size(0)\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        acc = accuracy()(preds, y)\n",
    "        self.log('val_loss', epoch_loss, prog_bar=True, on_epoch=True)\n",
    "        self.log('val_acc', acc, prog_bar=True, on_epoch=True)\n",
    "        \n",
    "        print('valid Loss: {:.4f} valid Acc: {:.4f}'.format(epoch_loss, acc))\n",
    "    \n",
    "    # New: テストデータに対するエポックごとの処理\n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        y_hat = torch.cat([val['y_hat'] for val in test_step_outputs], dim=0)\n",
    "        y = torch.cat([val['y'] for val in test_step_outputs], dim=0)\n",
    "        epoch_loss = sum([val['batch_loss'] for val in test_step_outputs]) / y_hat.size(0)\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        acc = accuracy()(preds, y)\n",
    "        self.log('test_loss', epoch_loss, prog_bar=True, on_epoch=True)\n",
    "        self.log('test_acc', acc, prog_bar=True, on_epoch=True)\n",
    "        \n",
    "        print('test Loss: {:.4f} test Acc: {:.4f}'.format(epoch_loss, acc))\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.SGD(self.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4, nesterov=True)\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60, 120, 160], gamma=0.2)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_loss'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-mentor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-melbourne",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-arthritis",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
