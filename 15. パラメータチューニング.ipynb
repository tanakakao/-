{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-supervision",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "involved-villa",
   "metadata": {},
   "source": [
    "## 15. パラメータチューニング\n",
    "機械学習には以下の2種類のパラメータがある。  \n",
    "- 訓練データから学習するパラメータ\n",
    "- 個別に最適化する学習アルゴリズムのパラメータ  \n",
    "\n",
    "後者はモデルのチューニングパラメータでり、**ハイパーパラメータ**と呼ばれる。  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "normal-journey",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-threshold",
   "metadata": {},
   "source": [
    "#### グリッドサーチ\n",
    "網羅的探索手法であり、さまざまなハイパーパラメータの値からなるリストを指定すると、それらの組み合わせごとにモデルの性能を評価する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "mature-motivation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe_svc = make_pipeline(StandardScaler(), SVC())\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "param_grid = [{'svc__C':param_range, 'svc__kernel':['linear']},\n",
    "              {'svc__C':param_range, 'svc__gamma':param_range,\n",
    "               'svc__kernel':['rbf']}]\n",
    "\n",
    "# refit=Trueで訓練データ全体を使って最適な設定で自動的に再適合する\n",
    "gridsearch = GridSearchCV(pipe_svc, param_grid, cv=5, verbose=0, refit=True)\n",
    "\n",
    "best_model = gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "growing-relay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.980952380952381\n",
      "{'svc__C': 1.0, 'svc__kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "print(best_model.best_score_)\n",
    "print(best_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-climb",
   "metadata": {},
   "source": [
    "#### ランダムサーチ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sought-johnson",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# refit=Trueで訓練データ全体を使って最適な設定で自動的に再適合する\n",
    "randomsearch = RandomizedSearchCV(pipe_svc, param_grid, cv=5, verbose=0, refit=True)\n",
    "\n",
    "best_model = randomsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "missing-indiana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9714285714285713\n",
      "{'svc__kernel': 'rbf', 'svc__gamma': 0.1, 'svc__C': 10.0}\n"
     ]
    }
   ],
   "source": [
    "print(best_model.best_score_)\n",
    "print(best_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-fiction",
   "metadata": {},
   "source": [
    "#### 複数モデルからの選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daily-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline([(\"scaler\", StandardScaler()), (\"classifier\",RandomForestClassifier())])\n",
    "\n",
    "search_space = [{\"classifier\": [SVC()],\n",
    "                 'classifier__C':param_range,\n",
    "                 'classifier__kernel':['linear']},\n",
    "                {\"classifier\": [RandomForestClassifier()],\n",
    "                 \"classifier__n_estimators\": [10, 100, 1000],\n",
    "                 \"classifier__max_features\": [1, 2, 3]}]\n",
    "\n",
    "gridsearch = GridSearchCV(pipe, search_space, cv=5, verbose=0)\n",
    "\n",
    "best_model = gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "removable-commission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.980952380952381\n",
      "{'classifier': SVC(kernel='linear'), 'classifier__C': 1.0, 'classifier__kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "print(best_model.best_score_)\n",
    "print(best_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-reservation",
   "metadata": {},
   "source": [
    "#### hyperopt\n",
    "https://qiita.com/nazoking@github/items/f67f92dc60001a43b7dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "exact-baghdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, stratify=target)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "mexican-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def score(params):\n",
    "    print(\"Training with params: \")\n",
    "    print(params)\n",
    "    num_round = int(params['n_estimators'])\n",
    "    # del params['n_estimators']\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_val, label=y_val)\n",
    "    watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n",
    "    gbm_model = xgb.train(params, dtrain,\n",
    "                          evals=watchlist,\n",
    "                          num_boost_round=num_round,\n",
    "                          early_stopping_rounds=10,\n",
    "                          verbose_eval=True)\n",
    "    predictions = gbm_model.predict(dvalid,\n",
    "                                    ntree_limit=gbm_model.best_iteration + 1)\n",
    "    score = accuracy_score(y_val, predictions)\n",
    "    # TODO: Add the importance for the selected features\n",
    "    print(\"\\tScore {0}\\n\\n\".format(score))\n",
    "    # The score function should return the loss (1-score)\n",
    "    # since the optimize function looks for the minimum\n",
    "    loss = 1 - score\n",
    "    return {'loss': loss, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "guilty-partition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.8, 'eta': 0.225, 'eval_metric': 'mlogloss', 'gamma': 0.8, 'max_depth': 6, 'min_child_weight': 4.0, 'n_estimators': 436.0, 'nthread': 4, 'num_class': 3, 'objective': 'multi:softmax', 'seed': 1, 'silent': 1, 'subsample': 0.65, 'tree_method': 'exact'}\n",
      "[00:43:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { n_estimators, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\teval-mlogloss:0.86087\ttrain-mlogloss:0.83430                                                                       \n",
      "\n",
      "[1]\teval-mlogloss:0.68518\ttrain-mlogloss:0.65754                                                                       \n",
      "\n",
      "[2]\teval-mlogloss:0.57350\ttrain-mlogloss:0.52292                                                                       \n",
      "\n",
      "[3]\teval-mlogloss:0.47115\ttrain-mlogloss:0.42771                                                                       \n",
      "\n",
      "[4]\teval-mlogloss:0.41197\ttrain-mlogloss:0.35070                                                                       \n",
      "\n",
      "[5]\teval-mlogloss:0.35304\ttrain-mlogloss:0.29246                                                                       \n",
      "\n",
      "[6]\teval-mlogloss:0.31750\ttrain-mlogloss:0.24589                                                                       \n",
      "\n",
      "[7]\teval-mlogloss:0.28691\ttrain-mlogloss:0.21180                                                                       \n",
      "\n",
      "[8]\teval-mlogloss:0.26200\ttrain-mlogloss:0.18972                                                                       \n",
      "\n",
      "[9]\teval-mlogloss:0.24906\ttrain-mlogloss:0.17281                                                                       \n",
      "\n",
      "[10]\teval-mlogloss:0.23976\ttrain-mlogloss:0.15802                                                                      \n",
      "\n",
      "[11]\teval-mlogloss:0.22667\ttrain-mlogloss:0.14632                                                                      \n",
      "\n",
      "[12]\teval-mlogloss:0.22361\ttrain-mlogloss:0.14405                                                                      \n",
      "\n",
      "[13]\teval-mlogloss:0.22632\ttrain-mlogloss:0.13871                                                                      \n",
      "\n",
      "[14]\teval-mlogloss:0.21953\ttrain-mlogloss:0.13314                                                                      \n",
      "\n",
      "[15]\teval-mlogloss:0.22074\ttrain-mlogloss:0.13187                                                                      \n",
      "\n",
      "[16]\teval-mlogloss:0.22240\ttrain-mlogloss:0.13187                                                                      \n",
      "\n",
      "[17]\teval-mlogloss:0.22121\ttrain-mlogloss:0.13186                                                                      \n",
      "\n",
      "[18]\teval-mlogloss:0.22151\ttrain-mlogloss:0.13186                                                                      \n",
      "\n",
      "[19]\teval-mlogloss:0.22300\ttrain-mlogloss:0.12889                                                                      \n",
      "\n",
      "[20]\teval-mlogloss:0.22398\ttrain-mlogloss:0.12545                                                                      \n",
      "\n",
      "[21]\teval-mlogloss:0.22514\ttrain-mlogloss:0.12546                                                                      \n",
      "\n",
      "[22]\teval-mlogloss:0.22557\ttrain-mlogloss:0.12545                                                                      \n",
      "\n",
      "[23]\teval-mlogloss:0.22161\ttrain-mlogloss:0.12222                                                                      \n",
      "\n",
      "[24]\teval-mlogloss:0.22186\ttrain-mlogloss:0.12222                                                                      \n",
      "\n",
      "[25]\teval-mlogloss:0.22002\ttrain-mlogloss:0.12215                                                                      \n",
      "\n",
      "[26]\teval-mlogloss:0.21891\ttrain-mlogloss:0.12213                                                                      \n",
      "\n",
      "[27]\teval-mlogloss:0.21962\ttrain-mlogloss:0.12214                                                                      \n",
      "\n",
      "[28]\teval-mlogloss:0.21801\ttrain-mlogloss:0.12213                                                                      \n",
      "\n",
      "[29]\teval-mlogloss:0.21671\ttrain-mlogloss:0.12214                                                                      \n",
      "\n",
      "[30]\teval-mlogloss:0.21891\ttrain-mlogloss:0.12213                                                                      \n",
      "\n",
      "[31]\teval-mlogloss:0.21936\ttrain-mlogloss:0.12214                                                                      \n",
      "\n",
      "[32]\teval-mlogloss:0.21870\ttrain-mlogloss:0.12214                                                                      \n",
      "\n",
      "[33]\teval-mlogloss:0.21858\ttrain-mlogloss:0.12216                                                                      \n",
      "\n",
      "[34]\teval-mlogloss:0.21796\ttrain-mlogloss:0.12214                                                                      \n",
      "\n",
      "[35]\teval-mlogloss:0.21602\ttrain-mlogloss:0.12215                                                                      \n",
      "\n",
      "[36]\teval-mlogloss:0.21684\ttrain-mlogloss:0.12213                                                                      \n",
      "\n",
      "[37]\teval-mlogloss:0.21938\ttrain-mlogloss:0.12214                                                                      \n",
      "\n",
      "[38]\teval-mlogloss:0.21825\ttrain-mlogloss:0.12213                                                                      \n",
      "\n",
      "\tScore 0.9333333333333333                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.6000000000000001, 'eta': 0.42500000000000004, 'eval_metric': 'mlogloss', 'gamma': 0.6000000000000001, 'max_depth': 1, 'min_child_weight': 6.0, 'n_estimators': 964.0, 'nthread': 4, 'num_class': 3, 'objective': 'multi:softmax', 'seed': 1, 'silent': 1, 'subsample': 0.65, 'tree_method': 'exact'}\n",
      "[00:43:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { n_estimators, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\teval-mlogloss:0.78567\ttrain-mlogloss:0.74825                                                                       \n",
      "\n",
      "[1]\teval-mlogloss:0.63912\ttrain-mlogloss:0.59857                                                                       \n",
      "\n",
      "[2]\teval-mlogloss:0.53841\ttrain-mlogloss:0.46353                                                                       \n",
      "\n",
      "[3]\teval-mlogloss:0.40821\ttrain-mlogloss:0.35394                                                                       \n",
      "\n",
      "[4]\teval-mlogloss:0.36045\ttrain-mlogloss:0.30280                                                                       \n",
      "\n",
      "[5]\teval-mlogloss:0.29198\ttrain-mlogloss:0.24591                                                                       \n",
      "\n",
      "[6]\teval-mlogloss:0.28448\ttrain-mlogloss:0.22550                                                                       \n",
      "\n",
      "[7]\teval-mlogloss:0.26653\ttrain-mlogloss:0.21045                                                                       \n",
      "\n",
      "[8]\teval-mlogloss:0.26734\ttrain-mlogloss:0.21033                                                                       \n",
      "\n",
      "[9]\teval-mlogloss:0.25031\ttrain-mlogloss:0.20225                                                                       \n",
      "\n",
      "[10]\teval-mlogloss:0.24997\ttrain-mlogloss:0.20240                                                                      \n",
      "\n",
      "[11]\teval-mlogloss:0.24933\ttrain-mlogloss:0.20219                                                                      \n",
      "\n",
      "[12]\teval-mlogloss:0.24983\ttrain-mlogloss:0.20015                                                                      \n",
      "\n",
      "[13]\teval-mlogloss:0.25250\ttrain-mlogloss:0.19993                                                                      \n",
      "\n",
      "[14]\teval-mlogloss:0.24999\ttrain-mlogloss:0.19989                                                                      \n",
      "\n",
      "[15]\teval-mlogloss:0.25006\ttrain-mlogloss:0.19989                                                                      \n",
      "\n",
      "[16]\teval-mlogloss:0.25206\ttrain-mlogloss:0.19986                                                                      \n",
      "\n",
      "[17]\teval-mlogloss:0.25036\ttrain-mlogloss:0.19984                                                                      \n",
      "\n",
      "[18]\teval-mlogloss:0.25007\ttrain-mlogloss:0.19985                                                                      \n",
      "\n",
      "[19]\teval-mlogloss:0.25080\ttrain-mlogloss:0.19989                                                                      \n",
      "\n",
      "[20]\teval-mlogloss:0.24620\ttrain-mlogloss:0.19614                                                                      \n",
      "\n",
      "[21]\teval-mlogloss:0.24775\ttrain-mlogloss:0.19618                                                                      \n",
      "\n",
      "[22]\teval-mlogloss:0.24550\ttrain-mlogloss:0.19117                                                                      \n",
      "\n",
      "[23]\teval-mlogloss:0.24565\ttrain-mlogloss:0.19128                                                                      \n",
      "\n",
      "[24]\teval-mlogloss:0.24593\ttrain-mlogloss:0.18735                                                                      \n",
      "\n",
      "[25]\teval-mlogloss:0.24320\ttrain-mlogloss:0.18704                                                                      \n",
      "\n",
      "[26]\teval-mlogloss:0.24335\ttrain-mlogloss:0.18695                                                                      \n",
      "\n",
      "[27]\teval-mlogloss:0.24571\ttrain-mlogloss:0.18691                                                                      \n",
      "\n",
      "[28]\teval-mlogloss:0.24431\ttrain-mlogloss:0.18692                                                                      \n",
      "\n",
      "[29]\teval-mlogloss:0.24281\ttrain-mlogloss:0.18700                                                                      \n",
      "\n",
      "[30]\teval-mlogloss:0.24601\ttrain-mlogloss:0.18692                                                                      \n",
      "\n",
      "[31]\teval-mlogloss:0.24653\ttrain-mlogloss:0.18696                                                                      \n",
      "\n",
      "[32]\teval-mlogloss:0.24580\ttrain-mlogloss:0.18700                                                                      \n",
      "\n",
      "[33]\teval-mlogloss:0.24565\ttrain-mlogloss:0.18707                                                                      \n",
      "\n",
      "[34]\teval-mlogloss:0.24396\ttrain-mlogloss:0.18696                                                                      \n",
      "\n",
      "[35]\teval-mlogloss:0.24105\ttrain-mlogloss:0.18718                                                                      \n",
      "\n",
      "[36]\teval-mlogloss:0.24295\ttrain-mlogloss:0.18698                                                                      \n",
      "\n",
      "\tScore 0.9333333333333333                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.9, 'eta': 0.42500000000000004, 'eval_metric': 'mlogloss', 'gamma': 0.8, 'max_depth': 3, 'min_child_weight': 5.0, 'n_estimators': 997.0, 'nthread': 4, 'num_class': 3, 'objective': 'multi:softmax', 'seed': 1, 'silent': 1, 'subsample': 0.55, 'tree_method': 'exact'}\n",
      "[00:43:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { n_estimators, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\teval-mlogloss:0.69801\ttrain-mlogloss:0.64868                                                                       \n",
      "\n",
      "[1]\teval-mlogloss:0.47739\ttrain-mlogloss:0.42754                                                                       \n",
      "\n",
      "[2]\teval-mlogloss:0.37343\ttrain-mlogloss:0.30299                                                                       \n",
      "\n",
      "[3]\teval-mlogloss:0.32453\ttrain-mlogloss:0.24174                                                                       \n",
      "\n",
      "[4]\teval-mlogloss:0.29368\ttrain-mlogloss:0.19240                                                                       \n",
      "\n",
      "[5]\teval-mlogloss:0.29128\ttrain-mlogloss:0.17999                                                                       \n",
      "\n",
      "[6]\teval-mlogloss:0.29098\ttrain-mlogloss:0.17983                                                                       \n",
      "\n",
      "[7]\teval-mlogloss:0.29255\ttrain-mlogloss:0.17967                                                                       \n",
      "\n",
      "[8]\teval-mlogloss:0.29463\ttrain-mlogloss:0.17957                                                                       \n",
      "\n",
      "[9]\teval-mlogloss:0.29359\ttrain-mlogloss:0.17098                                                                       \n",
      "\n",
      "[10]\teval-mlogloss:0.29478\ttrain-mlogloss:0.17107                                                                      \n",
      "\n",
      "[11]\teval-mlogloss:0.29632\ttrain-mlogloss:0.17091                                                                      \n",
      "\n",
      "[12]\teval-mlogloss:0.29660\ttrain-mlogloss:0.17092                                                                      \n",
      "\n",
      "[13]\teval-mlogloss:0.30046\ttrain-mlogloss:0.17111                                                                      \n",
      "\n",
      "[14]\teval-mlogloss:0.28028\ttrain-mlogloss:0.15606                                                                      \n",
      "\n",
      "[15]\teval-mlogloss:0.27893\ttrain-mlogloss:0.15610                                                                      \n",
      "\n",
      "[16]\teval-mlogloss:0.28104\ttrain-mlogloss:0.15620                                                                      \n",
      "\n",
      "[17]\teval-mlogloss:0.27787\ttrain-mlogloss:0.15619                                                                      \n",
      "\n",
      "[18]\teval-mlogloss:0.27662\ttrain-mlogloss:0.15617                                                                      \n",
      "\n",
      "[19]\teval-mlogloss:0.28026\ttrain-mlogloss:0.15615                                                                      \n",
      "\n",
      "[20]\teval-mlogloss:0.27919\ttrain-mlogloss:0.15615                                                                      \n",
      "\n",
      "[21]\teval-mlogloss:0.28069\ttrain-mlogloss:0.15606                                                                      \n",
      "\n",
      "[22]\teval-mlogloss:0.28198\ttrain-mlogloss:0.15616                                                                      \n",
      "\n",
      "[23]\teval-mlogloss:0.28126\ttrain-mlogloss:0.15607                                                                      \n",
      "\n",
      "[24]\teval-mlogloss:0.28137\ttrain-mlogloss:0.15608                                                                      \n",
      "\n",
      "[25]\teval-mlogloss:0.28094\ttrain-mlogloss:0.15607                                                                      \n",
      "\n",
      "[26]\teval-mlogloss:0.28167\ttrain-mlogloss:0.15609                                                                      \n",
      "\n",
      "[27]\teval-mlogloss:0.28258\ttrain-mlogloss:0.15613                                                                      \n",
      "\n",
      "[28]\teval-mlogloss:0.28054\ttrain-mlogloss:0.15607                                                                      \n",
      "\n",
      "[29]\teval-mlogloss:0.27802\ttrain-mlogloss:0.15607                                                                      \n",
      "\n",
      "[30]\teval-mlogloss:0.28109\ttrain-mlogloss:0.15607                                                                      \n",
      "\n",
      "[31]\teval-mlogloss:0.28101\ttrain-mlogloss:0.15607                                                                      \n",
      "\n",
      "\tScore 0.9333333333333333                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 1.0, 'eta': 0.125, 'eval_metric': 'mlogloss', 'gamma': 0.9, 'max_depth': 11, 'min_child_weight': 5.0, 'n_estimators': 632.0, 'nthread': 4, 'num_class': 3, 'objective': 'multi:softmax', 'seed': 1, 'silent': 1, 'subsample': 0.7000000000000001, 'tree_method': 'exact'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:43:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { n_estimators, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\teval-mlogloss:0.96094\ttrain-mlogloss:0.94558                                                                       \n",
      "\n",
      "[1]\teval-mlogloss:0.84651\ttrain-mlogloss:0.82288                                                                       \n",
      "\n",
      "[2]\teval-mlogloss:0.74959\ttrain-mlogloss:0.72111                                                                       \n",
      "\n",
      "[3]\teval-mlogloss:0.66960\ttrain-mlogloss:0.63604                                                                       \n",
      "\n",
      "[4]\teval-mlogloss:0.61621\ttrain-mlogloss:0.57054                                                                       \n",
      "\n",
      "[5]\teval-mlogloss:0.55310\ttrain-mlogloss:0.50670                                                                       \n",
      "\n",
      "[6]\teval-mlogloss:0.50655\ttrain-mlogloss:0.45160                                                                       \n",
      "\n",
      "[7]\teval-mlogloss:0.46304\ttrain-mlogloss:0.40398                                                                       \n",
      "\n",
      "[8]\teval-mlogloss:0.43108\ttrain-mlogloss:0.36432                                                                       \n",
      "\n",
      "[9]\teval-mlogloss:0.39944\ttrain-mlogloss:0.32969                                                                       \n",
      "\n",
      "[10]\teval-mlogloss:0.37950\ttrain-mlogloss:0.29932                                                                      \n",
      "\n",
      "[11]\teval-mlogloss:0.35277\ttrain-mlogloss:0.27288                                                                      \n",
      "\n",
      "[12]\teval-mlogloss:0.33910\ttrain-mlogloss:0.25034                                                                      \n",
      "\n",
      "[13]\teval-mlogloss:0.32964\ttrain-mlogloss:0.23402                                                                      \n",
      "\n",
      "[14]\teval-mlogloss:0.31420\ttrain-mlogloss:0.21543                                                                      \n",
      "\n",
      "[15]\teval-mlogloss:0.30478\ttrain-mlogloss:0.20040                                                                      \n",
      "\n",
      "[16]\teval-mlogloss:0.29361\ttrain-mlogloss:0.18987                                                                      \n",
      "\n",
      "[17]\teval-mlogloss:0.28651\ttrain-mlogloss:0.17979                                                                      \n",
      "\n",
      "[18]\teval-mlogloss:0.27395\ttrain-mlogloss:0.17132                                                                      \n",
      "\n",
      "[19]\teval-mlogloss:0.26895\ttrain-mlogloss:0.16735                                                                      \n",
      "\n",
      "[20]\teval-mlogloss:0.26404\ttrain-mlogloss:0.15928                                                                      \n",
      "\n",
      "[21]\teval-mlogloss:0.26426\ttrain-mlogloss:0.15929                                                                      \n",
      "\n",
      "[22]\teval-mlogloss:0.25989\ttrain-mlogloss:0.15552                                                                      \n",
      "\n",
      "[23]\teval-mlogloss:0.25636\ttrain-mlogloss:0.15277                                                                      \n",
      "\n",
      "[24]\teval-mlogloss:0.25701\ttrain-mlogloss:0.15278                                                                      \n",
      "\n",
      "[25]\teval-mlogloss:0.25641\ttrain-mlogloss:0.15277                                                                      \n",
      "\n",
      "[26]\teval-mlogloss:0.25597\ttrain-mlogloss:0.15277                                                                      \n",
      "\n",
      "[27]\teval-mlogloss:0.25653\ttrain-mlogloss:0.15030                                                                      \n",
      "\n",
      "[28]\teval-mlogloss:0.25613\ttrain-mlogloss:0.15029                                                                      \n",
      "\n",
      "[29]\teval-mlogloss:0.25609\ttrain-mlogloss:0.14791                                                                      \n",
      "\n",
      "[30]\teval-mlogloss:0.25716\ttrain-mlogloss:0.14791                                                                      \n",
      "\n",
      "[31]\teval-mlogloss:0.25675\ttrain-mlogloss:0.14791                                                                      \n",
      "\n",
      "[32]\teval-mlogloss:0.25284\ttrain-mlogloss:0.14541                                                                      \n",
      "\n",
      "[33]\teval-mlogloss:0.25258\ttrain-mlogloss:0.14541                                                                      \n",
      "\n",
      "[34]\teval-mlogloss:0.25222\ttrain-mlogloss:0.14541                                                                      \n",
      "\n",
      "[35]\teval-mlogloss:0.25200\ttrain-mlogloss:0.14541                                                                      \n",
      "\n",
      "[36]\teval-mlogloss:0.25202\ttrain-mlogloss:0.14541                                                                      \n",
      "\n",
      "[37]\teval-mlogloss:0.25366\ttrain-mlogloss:0.14542                                                                      \n",
      "\n",
      "[38]\teval-mlogloss:0.25316\ttrain-mlogloss:0.14542                                                                      \n",
      "\n",
      "[39]\teval-mlogloss:0.25262\ttrain-mlogloss:0.14542                                                                      \n",
      "\n",
      "[40]\teval-mlogloss:0.25283\ttrain-mlogloss:0.14541                                                                      \n",
      "\n",
      "[41]\teval-mlogloss:0.25293\ttrain-mlogloss:0.14541                                                                      \n",
      "\n",
      "[42]\teval-mlogloss:0.25350\ttrain-mlogloss:0.14542                                                                      \n",
      "\n",
      "[43]\teval-mlogloss:0.25393\ttrain-mlogloss:0.14543                                                                      \n",
      "\n",
      "[44]\teval-mlogloss:0.25005\ttrain-mlogloss:0.14318                                                                      \n",
      "\n",
      "[45]\teval-mlogloss:0.24993\ttrain-mlogloss:0.14319                                                                      \n",
      "\n",
      "[46]\teval-mlogloss:0.25039\ttrain-mlogloss:0.14100                                                                      \n",
      "\n",
      "[47]\teval-mlogloss:0.24995\ttrain-mlogloss:0.14100                                                                      \n",
      "\n",
      "[48]\teval-mlogloss:0.25124\ttrain-mlogloss:0.14102                                                                      \n",
      "\n",
      "[49]\teval-mlogloss:0.25150\ttrain-mlogloss:0.14101                                                                      \n",
      "\n",
      "[50]\teval-mlogloss:0.25230\ttrain-mlogloss:0.14104                                                                      \n",
      "\n",
      "[51]\teval-mlogloss:0.25244\ttrain-mlogloss:0.14105                                                                      \n",
      "\n",
      "[52]\teval-mlogloss:0.25215\ttrain-mlogloss:0.14104                                                                      \n",
      "\n",
      "[53]\teval-mlogloss:0.25230\ttrain-mlogloss:0.14105                                                                      \n",
      "\n",
      "[54]\teval-mlogloss:0.25152\ttrain-mlogloss:0.14103                                                                      \n",
      "\n",
      "[55]\teval-mlogloss:0.25236\ttrain-mlogloss:0.14105                                                                      \n",
      "\n",
      "[56]\teval-mlogloss:0.25257\ttrain-mlogloss:0.14105                                                                      \n",
      "\n",
      "[57]\teval-mlogloss:0.25321\ttrain-mlogloss:0.14107                                                                      \n",
      "\n",
      "\tScore 0.9333333333333333                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.7000000000000001, 'eta': 0.225, 'eval_metric': 'mlogloss', 'gamma': 0.6000000000000001, 'max_depth': 2, 'min_child_weight': 2.0, 'n_estimators': 999.0, 'nthread': 4, 'num_class': 3, 'objective': 'multi:softmax', 'seed': 1, 'silent': 1, 'subsample': 0.7000000000000001, 'tree_method': 'exact'}\n",
      "[00:43:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { n_estimators, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-mlogloss:0.86143\ttrain-mlogloss:0.83373                                                                       \n",
      "\n",
      "[1]\teval-mlogloss:0.75765\ttrain-mlogloss:0.72118                                                                       \n",
      "\n",
      "[2]\teval-mlogloss:0.62226\ttrain-mlogloss:0.58398                                                                       \n",
      "\n",
      "[3]\teval-mlogloss:0.51843\ttrain-mlogloss:0.47423                                                                       \n",
      "\n",
      "[4]\teval-mlogloss:0.45794\ttrain-mlogloss:0.40330                                                                       \n",
      "\n",
      "[5]\teval-mlogloss:0.38721\ttrain-mlogloss:0.33347                                                                       \n",
      "\n",
      "[6]\teval-mlogloss:0.34370\ttrain-mlogloss:0.27757                                                                       \n",
      "\n",
      "[7]\teval-mlogloss:0.31281\ttrain-mlogloss:0.24091                                                                       \n",
      "\n",
      "[8]\teval-mlogloss:0.29061\ttrain-mlogloss:0.21087                                                                       \n",
      "\n",
      "[9]\teval-mlogloss:0.27401\ttrain-mlogloss:0.18343                                                                       \n",
      "\n",
      "[10]\teval-mlogloss:0.26408\ttrain-mlogloss:0.16043                                                                      \n",
      "\n",
      "[11]\teval-mlogloss:0.25434\ttrain-mlogloss:0.14578                                                                      \n",
      "\n",
      "[12]\teval-mlogloss:0.23896\ttrain-mlogloss:0.13670                                                                      \n",
      "\n",
      "[13]\teval-mlogloss:0.24206\ttrain-mlogloss:0.12347                                                                      \n",
      "\n",
      "[14]\teval-mlogloss:0.23358\ttrain-mlogloss:0.11072                                                                      \n",
      "\n",
      "[15]\teval-mlogloss:0.22170\ttrain-mlogloss:0.10456                                                                      \n",
      "\n",
      "[16]\teval-mlogloss:0.21732\ttrain-mlogloss:0.09721                                                                      \n",
      "\n",
      "[17]\teval-mlogloss:0.21597\ttrain-mlogloss:0.08939                                                                      \n",
      "\n",
      "[18]\teval-mlogloss:0.21524\ttrain-mlogloss:0.08485                                                                      \n",
      "\n",
      "[19]\teval-mlogloss:0.21596\ttrain-mlogloss:0.08486                                                                      \n",
      "\n",
      "[20]\teval-mlogloss:0.21907\ttrain-mlogloss:0.08240                                                                      \n",
      "\n",
      "[21]\teval-mlogloss:0.21947\ttrain-mlogloss:0.08240                                                                      \n",
      "\n",
      "[22]\teval-mlogloss:0.21613\ttrain-mlogloss:0.08010                                                                      \n",
      "\n",
      "[23]\teval-mlogloss:0.21670\ttrain-mlogloss:0.08013                                                                      \n",
      "\n",
      "[24]\teval-mlogloss:0.21731\ttrain-mlogloss:0.08013                                                                      \n",
      "\n",
      "[25]\teval-mlogloss:0.21576\ttrain-mlogloss:0.08009                                                                      \n",
      "\n",
      "[26]\teval-mlogloss:0.21392\ttrain-mlogloss:0.08007                                                                      \n",
      "\n",
      "[27]\teval-mlogloss:0.21402\ttrain-mlogloss:0.08008                                                                      \n",
      "\n",
      "[28]\teval-mlogloss:0.21290\ttrain-mlogloss:0.08007                                                                      \n",
      "\n",
      "[29]\teval-mlogloss:0.21539\ttrain-mlogloss:0.07810                                                                      \n",
      "\n",
      "[30]\teval-mlogloss:0.21196\ttrain-mlogloss:0.07724                                                                      \n",
      "\n",
      "[31]\teval-mlogloss:0.21277\ttrain-mlogloss:0.07592                                                                      \n",
      "\n",
      "[32]\teval-mlogloss:0.20898\ttrain-mlogloss:0.07533                                                                      \n",
      "\n",
      "[33]\teval-mlogloss:0.20899\ttrain-mlogloss:0.07534                                                                      \n",
      "\n",
      "[34]\teval-mlogloss:0.20939\ttrain-mlogloss:0.07533                                                                      \n",
      "\n",
      "[35]\teval-mlogloss:0.21011\ttrain-mlogloss:0.07532                                                                      \n",
      "\n",
      "[36]\teval-mlogloss:0.21012\ttrain-mlogloss:0.07532                                                                      \n",
      "\n",
      "[37]\teval-mlogloss:0.21492\ttrain-mlogloss:0.07537                                                                      \n",
      "\n",
      "[38]\teval-mlogloss:0.21195\ttrain-mlogloss:0.07533                                                                      \n",
      "\n",
      "[39]\teval-mlogloss:0.20996\ttrain-mlogloss:0.07533                                                                      \n",
      "\n",
      "[40]\teval-mlogloss:0.21007\ttrain-mlogloss:0.07532                                                                      \n",
      "\n",
      "[41]\teval-mlogloss:0.21013\ttrain-mlogloss:0.07532                                                                      \n",
      "\n",
      "[42]\teval-mlogloss:0.20980\ttrain-mlogloss:0.07532                                                                      \n",
      "\n",
      "[43]\teval-mlogloss:0.21172\ttrain-mlogloss:0.07533                                                                      \n",
      "\n",
      "[44]\teval-mlogloss:0.21335\ttrain-mlogloss:0.07312                                                                      \n",
      "\n",
      "[45]\teval-mlogloss:0.21295\ttrain-mlogloss:0.07315                                                                      \n",
      "\n",
      "[46]\teval-mlogloss:0.21316\ttrain-mlogloss:0.07315                                                                      \n",
      "\n",
      "[47]\teval-mlogloss:0.21133\ttrain-mlogloss:0.07318                                                                      \n",
      "\n",
      "[48]\teval-mlogloss:0.21639\ttrain-mlogloss:0.07312                                                                      \n",
      "\n",
      "[49]\teval-mlogloss:0.21652\ttrain-mlogloss:0.07310                                                                      \n",
      "\n",
      "[50]\teval-mlogloss:0.21722\ttrain-mlogloss:0.07162                                                                      \n",
      "\n",
      "[51]\teval-mlogloss:0.21779\ttrain-mlogloss:0.07164                                                                      \n",
      "\n",
      "[52]\teval-mlogloss:0.21529\ttrain-mlogloss:0.07160                                                                      \n",
      "\n",
      "[53]\teval-mlogloss:0.21533\ttrain-mlogloss:0.07159                                                                      \n",
      "\n",
      "[54]\teval-mlogloss:0.21388\ttrain-mlogloss:0.07158                                                                      \n",
      "\n",
      "[55]\teval-mlogloss:0.21612\ttrain-mlogloss:0.07159                                                                      \n",
      "\n",
      "[56]\teval-mlogloss:0.21672\ttrain-mlogloss:0.07158                                                                      \n",
      "\n",
      "[57]\teval-mlogloss:0.21871\ttrain-mlogloss:0.07164                                                                      \n",
      "\n",
      "[58]\teval-mlogloss:0.21809\ttrain-mlogloss:0.07165                                                                      \n",
      "\n",
      "[59]\teval-mlogloss:0.21636\ttrain-mlogloss:0.07160                                                                      \n",
      "\n",
      "[60]\teval-mlogloss:0.21713\ttrain-mlogloss:0.07049                                                                      \n",
      "\n",
      "[61]\teval-mlogloss:0.21520\ttrain-mlogloss:0.07048                                                                      \n",
      "\n",
      "[62]\teval-mlogloss:0.21420\ttrain-mlogloss:0.07047                                                                      \n",
      "\n",
      "[63]\teval-mlogloss:0.21311\ttrain-mlogloss:0.07047                                                                      \n",
      "\n",
      "[64]\teval-mlogloss:0.21150\ttrain-mlogloss:0.07050                                                                      \n",
      "\n",
      "[65]\teval-mlogloss:0.20963\ttrain-mlogloss:0.07055                                                                      \n",
      "\n",
      "[66]\teval-mlogloss:0.21064\ttrain-mlogloss:0.07051                                                                      \n",
      "\n",
      "[67]\teval-mlogloss:0.21087\ttrain-mlogloss:0.07049                                                                      \n",
      "\n",
      "[68]\teval-mlogloss:0.20882\ttrain-mlogloss:0.07055                                                                      \n",
      "\n",
      "[69]\teval-mlogloss:0.20995\ttrain-mlogloss:0.07052                                                                      \n",
      "\n",
      "[70]\teval-mlogloss:0.21043\ttrain-mlogloss:0.07050                                                                      \n",
      "\n",
      "[71]\teval-mlogloss:0.20820\ttrain-mlogloss:0.07058                                                                      \n",
      "\n",
      "[72]\teval-mlogloss:0.21055\ttrain-mlogloss:0.07052                                                                      \n",
      "\n",
      "[73]\teval-mlogloss:0.21162\ttrain-mlogloss:0.07053                                                                      \n",
      "\n",
      "\tScore 0.9333333333333333                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.9500000000000001, 'eta': 0.5, 'eval_metric': 'mlogloss', 'gamma': 0.8, 'max_depth': 13, 'min_child_weight': 2.0, 'n_estimators': 958.0, 'nthread': 4, 'num_class': 3, 'objective': 'multi:softmax', 'seed': 1, 'silent': 1, 'subsample': 0.75, 'tree_method': 'exact'}\n",
      "[00:43:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { n_estimators, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\teval-mlogloss:0.62711\ttrain-mlogloss:0.56444                                                                       \n",
      "\n",
      "[1]\teval-mlogloss:0.41118\ttrain-mlogloss:0.35030                                                                       \n",
      "\n",
      "[2]\teval-mlogloss:0.32317\ttrain-mlogloss:0.22779                                                                       \n",
      "\n",
      "[3]\teval-mlogloss:0.26576\ttrain-mlogloss:0.16201                                                                       \n",
      "\n",
      "[4]\teval-mlogloss:0.25433\ttrain-mlogloss:0.11940                                                                       \n",
      "\n",
      "[5]\teval-mlogloss:0.23806\ttrain-mlogloss:0.09399                                                                       \n",
      "\n",
      "[6]\teval-mlogloss:0.23592\ttrain-mlogloss:0.08131                                                                       \n",
      "\n",
      "[7]\teval-mlogloss:0.22849\ttrain-mlogloss:0.08105                                                                       \n",
      "\n",
      "[8]\teval-mlogloss:0.22782\ttrain-mlogloss:0.08102                                                                       \n",
      "\n",
      "[9]\teval-mlogloss:0.23310\ttrain-mlogloss:0.07685                                                                       \n",
      "\n",
      "[10]\teval-mlogloss:0.23247\ttrain-mlogloss:0.07686                                                                      \n",
      "\n",
      "[11]\teval-mlogloss:0.23140\ttrain-mlogloss:0.07683                                                                      \n",
      "\n",
      "[12]\teval-mlogloss:0.22977\ttrain-mlogloss:0.07687                                                                      \n",
      "\n",
      "[13]\teval-mlogloss:0.24575\ttrain-mlogloss:0.07311                                                                      \n",
      "\n",
      "[14]\teval-mlogloss:0.24465\ttrain-mlogloss:0.07310                                                                      \n",
      "\n",
      "[15]\teval-mlogloss:0.24527\ttrain-mlogloss:0.07310                                                                      \n",
      "\n",
      "[16]\teval-mlogloss:0.24796\ttrain-mlogloss:0.07317                                                                      \n",
      "\n",
      "[17]\teval-mlogloss:0.24051\ttrain-mlogloss:0.07311                                                                      \n",
      "\n",
      "[18]\teval-mlogloss:0.24625\ttrain-mlogloss:0.07313                                                                      \n",
      "\n",
      "[19]\teval-mlogloss:0.24367\ttrain-mlogloss:0.07309                                                                      \n",
      "\n",
      "[20]\teval-mlogloss:0.24259\ttrain-mlogloss:0.07314                                                                      \n",
      "\n",
      "[21]\teval-mlogloss:0.24203\ttrain-mlogloss:0.07307                                                                      \n",
      "\n",
      "[22]\teval-mlogloss:0.23528\ttrain-mlogloss:0.06972                                                                      \n",
      "\n",
      "[23]\teval-mlogloss:0.23311\ttrain-mlogloss:0.06967                                                                      \n",
      "\n",
      "[24]\teval-mlogloss:0.23511\ttrain-mlogloss:0.06972                                                                      \n",
      "\n",
      "[25]\teval-mlogloss:0.22741\ttrain-mlogloss:0.06968                                                                      \n",
      "\n",
      "[26]\teval-mlogloss:0.22915\ttrain-mlogloss:0.06965                                                                      \n",
      "\n",
      "[27]\teval-mlogloss:0.22954\ttrain-mlogloss:0.06966                                                                      \n",
      "\n",
      "[28]\teval-mlogloss:0.22771\ttrain-mlogloss:0.06967                                                                      \n",
      "\n",
      "[29]\teval-mlogloss:0.22502\ttrain-mlogloss:0.06973                                                                      \n",
      "\n",
      "[30]\teval-mlogloss:0.23302\ttrain-mlogloss:0.06966                                                                      \n",
      "\n",
      "[31]\teval-mlogloss:0.22869\ttrain-mlogloss:0.06967                                                                      \n",
      "\n",
      "[32]\teval-mlogloss:0.22586\ttrain-mlogloss:0.06972                                                                      \n",
      "\n",
      "[33]\teval-mlogloss:0.22622\ttrain-mlogloss:0.06971                                                                      \n",
      "\n",
      "[34]\teval-mlogloss:0.22923\ttrain-mlogloss:0.06965                                                                      \n",
      "\n",
      "[35]\teval-mlogloss:0.23197\ttrain-mlogloss:0.06966                                                                      \n",
      "\n",
      "[36]\teval-mlogloss:0.23235\ttrain-mlogloss:0.06966                                                                      \n",
      "\n",
      "\tScore 0.9333333333333333                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.55, 'eta': 0.25, 'eval_metric': 'mlogloss', 'gamma': 0.65, 'max_depth': 5, 'min_child_weight': 2.0, 'n_estimators': 986.0, 'nthread': 4, 'num_class': 3, 'objective': 'multi:softmax', 'seed': 1, 'silent': 1, 'subsample': 0.8500000000000001, 'tree_method': 'exact'}\n",
      "[00:43:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { n_estimators, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\teval-mlogloss:0.83151\ttrain-mlogloss:0.80111                                                                       \n",
      "\n",
      "[1]\teval-mlogloss:0.73097\ttrain-mlogloss:0.67470                                                                       \n",
      "\n",
      "[2]\teval-mlogloss:0.60463\ttrain-mlogloss:0.53053                                                                       \n",
      "\n",
      "[3]\teval-mlogloss:0.49732\ttrain-mlogloss:0.41946                                                                       \n",
      "\n",
      "[4]\teval-mlogloss:0.44701\ttrain-mlogloss:0.35968                                                                       \n",
      "\n",
      "[5]\teval-mlogloss:0.38328\ttrain-mlogloss:0.29253                                                                       \n",
      "\n",
      "[6]\teval-mlogloss:0.33197\ttrain-mlogloss:0.24132                                                                       \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7]\teval-mlogloss:0.30250\ttrain-mlogloss:0.20673                                                                       \n",
      "\n",
      "[8]\teval-mlogloss:0.28296\ttrain-mlogloss:0.17966                                                                       \n",
      "\n",
      "[9]\teval-mlogloss:0.26905\ttrain-mlogloss:0.15435                                                                       \n",
      "\n",
      "[10]\teval-mlogloss:0.25194\ttrain-mlogloss:0.13348                                                                      \n",
      "\n",
      "[11]\teval-mlogloss:0.24354\ttrain-mlogloss:0.12036                                                                      \n",
      "\n",
      "[12]\teval-mlogloss:0.23131\ttrain-mlogloss:0.11147                                                                      \n",
      "\n",
      "[13]\teval-mlogloss:0.23566\ttrain-mlogloss:0.10098                                                                      \n",
      "\n",
      "[14]\teval-mlogloss:0.22910\ttrain-mlogloss:0.09148                                                                      \n",
      "\n",
      "[15]\teval-mlogloss:0.21883\ttrain-mlogloss:0.08489                                                                      \n",
      "\n",
      "[16]\teval-mlogloss:0.21270\ttrain-mlogloss:0.08036                                                                      \n",
      "\n",
      "[17]\teval-mlogloss:0.21077\ttrain-mlogloss:0.07529                                                                      \n",
      "\n",
      "[18]\teval-mlogloss:0.21587\ttrain-mlogloss:0.07334                                                                      \n",
      "\n",
      "[19]\teval-mlogloss:0.21861\ttrain-mlogloss:0.07149                                                                      \n",
      "\n",
      "[20]\teval-mlogloss:0.22232\ttrain-mlogloss:0.06971                                                                      \n",
      "\n",
      "[21]\teval-mlogloss:0.22133\ttrain-mlogloss:0.06969                                                                      \n",
      "\n",
      "[22]\teval-mlogloss:0.22036\ttrain-mlogloss:0.06968                                                                      \n",
      "\n",
      "[23]\teval-mlogloss:0.21996\ttrain-mlogloss:0.06967                                                                      \n",
      "\n",
      "[24]\teval-mlogloss:0.22077\ttrain-mlogloss:0.06969                                                                      \n",
      "\n",
      "[25]\teval-mlogloss:0.21859\ttrain-mlogloss:0.06967                                                                      \n",
      "\n",
      "[26]\teval-mlogloss:0.21896\ttrain-mlogloss:0.06967                                                                      \n",
      "\n",
      "[27]\teval-mlogloss:0.21949\ttrain-mlogloss:0.06967                                                                      \n",
      "\n",
      "[28]\teval-mlogloss:0.21871\ttrain-mlogloss:0.06966                                                                      \n",
      "\n",
      "[29]\teval-mlogloss:0.22130\ttrain-mlogloss:0.06789                                                                      \n",
      "\n",
      "[30]\teval-mlogloss:0.21698\ttrain-mlogloss:0.06710                                                                      \n",
      "\n",
      "[31]\teval-mlogloss:0.21640\ttrain-mlogloss:0.06709                                                                      \n",
      "\n",
      "[32]\teval-mlogloss:0.21432\ttrain-mlogloss:0.06706                                                                      \n",
      "\n",
      "[33]\teval-mlogloss:0.21276\ttrain-mlogloss:0.06706                                                                      \n",
      "\n",
      "[34]\teval-mlogloss:0.21470\ttrain-mlogloss:0.06707                                                                      \n",
      "\n",
      "[35]\teval-mlogloss:0.21764\ttrain-mlogloss:0.06712                                                                      \n",
      "\n",
      "[36]\teval-mlogloss:0.21625\ttrain-mlogloss:0.06709                                                                      \n",
      "\n",
      "[37]\teval-mlogloss:0.21709\ttrain-mlogloss:0.06710                                                                      \n",
      "\n",
      "[38]\teval-mlogloss:0.21514\ttrain-mlogloss:0.06707                                                                      \n",
      "\n",
      "[39]\teval-mlogloss:0.21377\ttrain-mlogloss:0.06707                                                                      \n",
      "\n",
      "[40]\teval-mlogloss:0.21309\ttrain-mlogloss:0.06707                                                                      \n",
      "\n",
      "[41]\teval-mlogloss:0.21367\ttrain-mlogloss:0.06708                                                                      \n",
      "\n",
      "[42]\teval-mlogloss:0.21212\ttrain-mlogloss:0.06707                                                                      \n",
      "\n",
      "\tScore 0.9333333333333333                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.65, 'eta': 0.2, 'eval_metric': 'mlogloss', 'gamma': 0.65, 'max_depth': 3, 'min_child_weight': 3.0, 'n_estimators': 295.0, 'nthread': 4, 'num_class': 3, 'objective': 'multi:softmax', 'seed': 1, 'silent': 1, 'subsample': 0.9500000000000001, 'tree_method': 'exact'}\n",
      "[00:43:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { n_estimators, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\teval-mlogloss:0.87952\ttrain-mlogloss:0.85552                                                                       \n",
      "\n",
      "[1]\teval-mlogloss:0.77836\ttrain-mlogloss:0.74575                                                                       \n",
      "\n",
      "[2]\teval-mlogloss:0.66290\ttrain-mlogloss:0.61343                                                                       \n",
      "\n",
      "[3]\teval-mlogloss:0.55923\ttrain-mlogloss:0.50336                                                                       \n",
      "\n",
      "[4]\teval-mlogloss:0.49786\ttrain-mlogloss:0.43356                                                                       \n",
      "\n",
      "[5]\teval-mlogloss:0.43381\ttrain-mlogloss:0.36458                                                                       \n",
      "\n",
      "[6]\teval-mlogloss:0.37957\ttrain-mlogloss:0.30858                                                                       \n",
      "\n",
      "[7]\teval-mlogloss:0.34547\ttrain-mlogloss:0.26814                                                                       \n",
      "\n",
      "[8]\teval-mlogloss:0.31506\ttrain-mlogloss:0.23580                                                                       \n",
      "\n",
      "[9]\teval-mlogloss:0.29471\ttrain-mlogloss:0.20486                                                                       \n",
      "\n",
      "[10]\teval-mlogloss:0.27959\ttrain-mlogloss:0.17943                                                                      \n",
      "\n",
      "[11]\teval-mlogloss:0.26797\ttrain-mlogloss:0.16293                                                                      \n",
      "\n",
      "[12]\teval-mlogloss:0.25426\ttrain-mlogloss:0.15085                                                                      \n",
      "\n",
      "[13]\teval-mlogloss:0.25188\ttrain-mlogloss:0.13688                                                                      \n",
      "\n",
      "[14]\teval-mlogloss:0.24922\ttrain-mlogloss:0.12341                                                                      \n",
      "\n",
      "[15]\teval-mlogloss:0.24730\ttrain-mlogloss:0.11219                                                                      \n",
      "\n",
      "[16]\teval-mlogloss:0.23548\ttrain-mlogloss:0.10419                                                                      \n",
      "\n",
      "[17]\teval-mlogloss:0.23411\ttrain-mlogloss:0.09704                                                                      \n",
      "\n",
      "[18]\teval-mlogloss:0.22788\ttrain-mlogloss:0.09183                                                                      \n",
      "\n",
      "[19]\teval-mlogloss:0.22815\ttrain-mlogloss:0.08741                                                                      \n",
      "\n",
      "[20]\teval-mlogloss:0.23078\ttrain-mlogloss:0.08523                                                                      \n",
      "\n",
      "[21]\teval-mlogloss:0.23289\ttrain-mlogloss:0.08316                                                                      \n",
      "\n",
      "[22]\teval-mlogloss:0.23002\ttrain-mlogloss:0.08130                                                                      \n",
      "\n",
      "[23]\teval-mlogloss:0.22695\ttrain-mlogloss:0.07982                                                                      \n",
      "\n",
      "[24]\teval-mlogloss:0.22911\ttrain-mlogloss:0.07834                                                                      \n",
      "\n",
      "[25]\teval-mlogloss:0.22872\ttrain-mlogloss:0.07834                                                                      \n",
      "\n",
      "[26]\teval-mlogloss:0.22893\ttrain-mlogloss:0.07834                                                                      \n",
      "\n",
      "[27]\teval-mlogloss:0.23135\ttrain-mlogloss:0.07703                                                                      \n",
      "\n",
      "[28]\teval-mlogloss:0.23136\ttrain-mlogloss:0.07703                                                                      \n",
      "\n",
      "[29]\teval-mlogloss:0.23168\ttrain-mlogloss:0.07703                                                                      \n",
      "\n",
      "[30]\teval-mlogloss:0.22941\ttrain-mlogloss:0.07579                                                                      \n",
      "\n",
      "[31]\teval-mlogloss:0.22908\ttrain-mlogloss:0.07578                                                                      \n",
      "\n",
      "[32]\teval-mlogloss:0.22931\ttrain-mlogloss:0.07578                                                                      \n",
      "\n",
      "[33]\teval-mlogloss:0.22807\ttrain-mlogloss:0.07577                                                                      \n",
      "\n",
      "[34]\teval-mlogloss:0.22878\ttrain-mlogloss:0.07577                                                                      \n",
      "\n",
      "[35]\teval-mlogloss:0.23174\ttrain-mlogloss:0.07465                                                                      \n",
      "\n",
      "[36]\teval-mlogloss:0.23077\ttrain-mlogloss:0.07463                                                                      \n",
      "\n",
      "[37]\teval-mlogloss:0.23058\ttrain-mlogloss:0.07463                                                                      \n",
      "\n",
      "[38]\teval-mlogloss:0.23013\ttrain-mlogloss:0.07462                                                                      \n",
      "\n",
      "[39]\teval-mlogloss:0.22915\ttrain-mlogloss:0.07462                                                                      \n",
      "\n",
      "[40]\teval-mlogloss:0.22933\ttrain-mlogloss:0.07462                                                                      \n",
      "\n",
      "[41]\teval-mlogloss:0.22906\ttrain-mlogloss:0.07462                                                                      \n",
      "\n",
      "[42]\teval-mlogloss:0.22878\ttrain-mlogloss:0.07462                                                                      \n",
      "\n",
      "[43]\teval-mlogloss:0.22881\ttrain-mlogloss:0.07462                                                                      \n",
      "\n",
      "[44]\teval-mlogloss:0.22919\ttrain-mlogloss:0.07462                                                                      \n",
      "\n",
      "[45]\teval-mlogloss:0.22911\ttrain-mlogloss:0.07462                                                                      \n",
      "\n",
      "[46]\teval-mlogloss:0.22918\ttrain-mlogloss:0.07462                                                                      \n",
      "\n",
      "[47]\teval-mlogloss:0.22890\ttrain-mlogloss:0.07462                                                                      \n",
      "\n",
      "[48]\teval-mlogloss:0.22922\ttrain-mlogloss:0.07462                                                                      \n",
      "\n",
      "[49]\teval-mlogloss:0.22843\ttrain-mlogloss:0.07462                                                                      \n",
      "\n",
      "[50]\teval-mlogloss:0.22957\ttrain-mlogloss:0.07462                                                                      \n",
      "\n",
      "[51]\teval-mlogloss:0.23037\ttrain-mlogloss:0.07463                                                                      \n",
      "\n",
      "[52]\teval-mlogloss:0.23139\ttrain-mlogloss:0.07464                                                                      \n",
      "\n",
      "[53]\teval-mlogloss:0.23093\ttrain-mlogloss:0.07463                                                                      \n",
      "\n",
      "[54]\teval-mlogloss:0.23055\ttrain-mlogloss:0.07463                                                                      \n",
      "\n",
      "\tScore 0.9333333333333333                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.65, 'eta': 0.42500000000000004, 'eval_metric': 'mlogloss', 'gamma': 0.75, 'max_depth': 8, 'min_child_weight': 5.0, 'n_estimators': 121.0, 'nthread': 4, 'num_class': 3, 'objective': 'multi:softmax', 'seed': 1, 'silent': 1, 'subsample': 0.9500000000000001, 'tree_method': 'exact'}\n",
      "[00:43:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { n_estimators, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\teval-mlogloss:0.68237\ttrain-mlogloss:0.63044                                                                       \n",
      "\n",
      "[1]\teval-mlogloss:0.55737\ttrain-mlogloss:0.49464                                                                       \n",
      "\n",
      "[2]\teval-mlogloss:0.43405\ttrain-mlogloss:0.34340                                                                       \n",
      "\n",
      "[3]\teval-mlogloss:0.34005\ttrain-mlogloss:0.24299                                                                       \n",
      "\n",
      "[4]\teval-mlogloss:0.30824\ttrain-mlogloss:0.19664                                                                       \n",
      "\n",
      "[5]\teval-mlogloss:0.27238\ttrain-mlogloss:0.15454                                                                       \n",
      "\n",
      "[6]\teval-mlogloss:0.24461\ttrain-mlogloss:0.12793                                                                       \n",
      "\n",
      "[7]\teval-mlogloss:0.23271\ttrain-mlogloss:0.11945                                                                       \n",
      "\n",
      "[8]\teval-mlogloss:0.22354\ttrain-mlogloss:0.11299                                                                       \n",
      "\n",
      "[9]\teval-mlogloss:0.22298\ttrain-mlogloss:0.11299                                                                       \n",
      "\n",
      "[10]\teval-mlogloss:0.22303\ttrain-mlogloss:0.11298                                                                      \n",
      "\n",
      "[11]\teval-mlogloss:0.22334\ttrain-mlogloss:0.11297                                                                      \n",
      "\n",
      "[12]\teval-mlogloss:0.22323\ttrain-mlogloss:0.11297                                                                      \n",
      "\n",
      "[13]\teval-mlogloss:0.22553\ttrain-mlogloss:0.11301                                                                      \n",
      "\n",
      "[14]\teval-mlogloss:0.22493\ttrain-mlogloss:0.11299                                                                      \n",
      "\n",
      "[15]\teval-mlogloss:0.22426\ttrain-mlogloss:0.11298                                                                      \n",
      "\n",
      "[16]\teval-mlogloss:0.22420\ttrain-mlogloss:0.11298                                                                      \n",
      "\n",
      "[17]\teval-mlogloss:0.22354\ttrain-mlogloss:0.11297                                                                      \n",
      "\n",
      "[18]\teval-mlogloss:0.22792\ttrain-mlogloss:0.10755                                                                      \n",
      "\n",
      "[19]\teval-mlogloss:0.22861\ttrain-mlogloss:0.10756                                                                      \n",
      "\n",
      "[20]\teval-mlogloss:0.22905\ttrain-mlogloss:0.10757                                                                      \n",
      "\n",
      "[21]\teval-mlogloss:0.22790\ttrain-mlogloss:0.10756                                                                      \n",
      "\n",
      "[22]\teval-mlogloss:0.22847\ttrain-mlogloss:0.10756                                                                      \n",
      "\n",
      "[23]\teval-mlogloss:0.22764\ttrain-mlogloss:0.10755                                                                      \n",
      "\n",
      "[24]\teval-mlogloss:0.22782\ttrain-mlogloss:0.10755                                                                      \n",
      "\n",
      "[25]\teval-mlogloss:0.22718\ttrain-mlogloss:0.10755                                                                      \n",
      "\n",
      "[26]\teval-mlogloss:0.22794\ttrain-mlogloss:0.10756                                                                      \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27]\teval-mlogloss:0.22791\ttrain-mlogloss:0.10756                                                                      \n",
      "\n",
      "[28]\teval-mlogloss:0.22793\ttrain-mlogloss:0.10757                                                                      \n",
      "\n",
      "\tScore 0.9333333333333333                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.6000000000000001, 'eta': 0.375, 'eval_metric': 'mlogloss', 'gamma': 0.65, 'max_depth': 2, 'min_child_weight': 5.0, 'n_estimators': 589.0, 'nthread': 4, 'num_class': 3, 'objective': 'multi:softmax', 'seed': 1, 'silent': 1, 'subsample': 0.8, 'tree_method': 'exact'}\n",
      "[00:43:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { n_estimators, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\teval-mlogloss:0.72619\ttrain-mlogloss:0.68092                                                                       \n",
      "\n",
      "[1]\teval-mlogloss:0.60605\ttrain-mlogloss:0.54620                                                                       \n",
      "\n",
      "[2]\teval-mlogloss:0.47862\ttrain-mlogloss:0.39332                                                                       \n",
      "\n",
      "[3]\teval-mlogloss:0.37644\ttrain-mlogloss:0.28701                                                                       \n",
      "\n",
      "[4]\teval-mlogloss:0.33738\ttrain-mlogloss:0.23691                                                                       \n",
      "\n",
      "[5]\teval-mlogloss:0.29381\ttrain-mlogloss:0.18530                                                                       \n",
      "\n",
      "[6]\teval-mlogloss:0.28078\ttrain-mlogloss:0.16062                                                                       \n",
      "\n",
      "[7]\teval-mlogloss:0.26642\ttrain-mlogloss:0.14680                                                                       \n",
      "\n",
      "[8]\teval-mlogloss:0.25549\ttrain-mlogloss:0.13825                                                                       \n",
      "\n",
      "[9]\teval-mlogloss:0.24603\ttrain-mlogloss:0.13153                                                                       \n",
      "\n",
      "[10]\teval-mlogloss:0.24501\ttrain-mlogloss:0.13161                                                                      \n",
      "\n",
      "[11]\teval-mlogloss:0.24485\ttrain-mlogloss:0.13158                                                                      \n",
      "\n",
      "[12]\teval-mlogloss:0.23517\ttrain-mlogloss:0.12838                                                                      \n",
      "\n",
      "[13]\teval-mlogloss:0.23925\ttrain-mlogloss:0.12827                                                                      \n",
      "\n",
      "[14]\teval-mlogloss:0.23875\ttrain-mlogloss:0.12825                                                                      \n",
      "\n",
      "[15]\teval-mlogloss:0.23861\ttrain-mlogloss:0.12826                                                                      \n",
      "\n",
      "[16]\teval-mlogloss:0.23934\ttrain-mlogloss:0.12827                                                                      \n",
      "\n",
      "[17]\teval-mlogloss:0.23706\ttrain-mlogloss:0.12824                                                                      \n",
      "\n",
      "[18]\teval-mlogloss:0.23920\ttrain-mlogloss:0.12827                                                                      \n",
      "\n",
      "[19]\teval-mlogloss:0.23870\ttrain-mlogloss:0.12825                                                                      \n",
      "\n",
      "[20]\teval-mlogloss:0.23752\ttrain-mlogloss:0.12824                                                                      \n",
      "\n",
      "[21]\teval-mlogloss:0.23826\ttrain-mlogloss:0.12824                                                                      \n",
      "\n",
      "[22]\teval-mlogloss:0.23946\ttrain-mlogloss:0.12828                                                                      \n",
      "\n",
      "[23]\teval-mlogloss:0.23845\ttrain-mlogloss:0.12826                                                                      \n",
      "\n",
      "[24]\teval-mlogloss:0.23974\ttrain-mlogloss:0.12833                                                                      \n",
      "\n",
      "[25]\teval-mlogloss:0.23713\ttrain-mlogloss:0.12830                                                                      \n",
      "\n",
      "[26]\teval-mlogloss:0.23840\ttrain-mlogloss:0.12827                                                                      \n",
      "\n",
      "[27]\teval-mlogloss:0.23830\ttrain-mlogloss:0.12831                                                                      \n",
      "\n",
      "[28]\teval-mlogloss:0.23796\ttrain-mlogloss:0.12827                                                                      \n",
      "\n",
      "[29]\teval-mlogloss:0.23775\ttrain-mlogloss:0.12828                                                                      \n",
      "\n",
      "[30]\teval-mlogloss:0.24008\ttrain-mlogloss:0.12829                                                                      \n",
      "\n",
      "\tScore 0.9333333333333333                                                                                              \n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████████| 10/10 [00:03<00:00,  2.71trial/s, best loss: 0.06666666666666665]\n"
     ]
    }
   ],
   "source": [
    "param_space = {\n",
    "        'n_estimators': hp.quniform('n_estimators', 100, 1000, 1),\n",
    "        'eta': hp.quniform('eta', 0.025, 0.5, 0.025),\n",
    "        # A problem with max_depth casted to float instead of int with\n",
    "        # the hp.quniform method.\n",
    "        'max_depth':  hp.choice('max_depth', np.arange(1, 14, dtype=int)),\n",
    "        'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n",
    "        'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "        'gamma': hp.quniform('gamma', 0.5, 1, 0.05),\n",
    "        'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'objective': 'multi:softmax',\n",
    "        #'objective': 'multi:softprob',\n",
    "        # Increase this number if you have more cores. Otherwise, remove it and it will default \n",
    "        # to the maxium number. \n",
    "        'nthread': 4,\n",
    "        'booster': 'gbtree',\n",
    "        'tree_method': 'exact',\n",
    "        'silent': 1,\n",
    "        'num_class': 3,\n",
    "        'seed': 1\n",
    "    }\n",
    "\n",
    "max_evals= 10\n",
    "trials = Trials()\n",
    "history = []\n",
    "best = fmin(score, param_space, algo=tpe.suggest,\n",
    "            trials=trials,\n",
    "            max_evals=max_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "enabling-husband",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'booster': 'gbtree',\n",
       " 'colsample_bytree': 0.8,\n",
       " 'eta': 0.225,\n",
       " 'eval_metric': 'mlogloss',\n",
       " 'gamma': 0.8,\n",
       " 'max_depth': 6,\n",
       " 'min_child_weight': 4.0,\n",
       " 'n_estimators': 436.0,\n",
       " 'nthread': 4,\n",
       " 'num_class': 3,\n",
       " 'objective': 'multi:softmax',\n",
       " 'seed': 1,\n",
       " 'silent': 1,\n",
       " 'subsample': 0.65,\n",
       " 'tree_method': 'exact'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hyperopt import space_eval\n",
    "\n",
    "space_eval(param_space, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-fifteen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "popular-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def objective(X, y, args):\n",
    "    \"\"\"最小化したい目的関数\"\"\"\n",
    "    classifiers = {\n",
    "        'svm': SVC,\n",
    "        'rf': RandomForestClassifier,\n",
    "        'logit': LogisticRegression,\n",
    "    }\n",
    "    classifier = classifiers.get(args['model_type'])\n",
    "    del args['model_type']\n",
    "    model = classifier(**args)\n",
    "    # Stratified 5 Fold Cross Validation\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_validate(model, X=X, y=y, cv=kf)\n",
    "    # 最小化なので符号を反転する\n",
    "    return -1 * scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "excessive-machine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 100/100 [02:50<00:00,  1.70s/trial, best loss: -0.9904761904761905]\n",
      "{'C': 96.38239880971929, 'gamma': 0.007667030695280854, 'model_type': 'svm'}\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from hyperopt.pyll.base import scope\n",
    "\n",
    "f = partial(objective, X_train, y_train)\n",
    "# 変数の値域を定義する\n",
    "space = hp.choice('algorithms', [\n",
    "    {\n",
    "        'model_type': 'rf',\n",
    "        'n_estimators': scope.int(hp.uniform('n_estimators', 1e+1, 1e+3)),\n",
    "        'max_depth': scope.int(hp.uniform('max_depth', 1e+1, 1e+3)),\n",
    "    },\n",
    "    {\n",
    "        'model_type': 'svm',\n",
    "        'C': hp.uniform('C', 1e+0, 1e+2),\n",
    "        'gamma': hp.lognormal('gamma', 1e-2, 1e+1),\n",
    "    },\n",
    "    {\n",
    "        'model_type': 'logit',\n",
    "        'solver': 'lbfgs',\n",
    "        'multi_class': 'auto',\n",
    "        'max_iter': 1000,\n",
    "    }\n",
    "])\n",
    "# 探索過程を記録するオブジェクト\n",
    "trials = Trials()\n",
    "# 目的関数を最小化するパラメータを探索する\n",
    "best = fmin(fn=f, space=space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "# 結果を出力する\n",
    "print(space_eval(space, best))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-contributor",
   "metadata": {},
   "source": [
    "#### optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "legal-conclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'kernel': 'rbf', 'C': 1.3098008537935169, 'gamma': 0.39345898625869047}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from functools import partial\n",
    "\n",
    "def objective(X, y, trial):\n",
    "    \"\"\"最小化する目的関数\"\"\"\n",
    "    params = {\n",
    "        'kernel': trial.suggest_categorical('kernel', ['rbf', 'sigmoid', 'linear']),\n",
    "        'C': trial.suggest_loguniform('C', 1e+0, 1e+2),\n",
    "        'gamma': trial.suggest_loguniform('gamma', 1e-2, 1e+1),\n",
    "    }\n",
    "\n",
    "    # モデルを作る\n",
    "    model = SVC(**params)\n",
    "\n",
    "    # 5-Fold CV / Accuracy でモデルを評価する\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_validate(model, X=X, y=y, cv=kf)\n",
    "    # 最小化なので 1.0 からスコアを引く\n",
    "    return 1.0 - scores['test_score'].mean()\n",
    "\n",
    "\n",
    "# 目的関数にデータを適用する\n",
    "f = partial(objective, X_train, y_train)\n",
    "# 最適化のセッションを作る\n",
    "study = optuna.create_study()\n",
    "# 100 回試行する\n",
    "study.optimize(f, n_trials=100)\n",
    "# 最適化したパラメータを出力する\n",
    "print('params:', study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "three-ocean",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555555555555556"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=study.best_params\n",
    "model = SVC(**d)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-gambling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "vertical-sherman",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.logging.disable_default_handler()\n",
    "\n",
    "def objective(X, y, trial):\n",
    "    \"\"\"最小化する目的関数\"\"\"\n",
    "\n",
    "    # 使う分類器は SVM or RF\n",
    "    classifier = trial.suggest_categorical('classifier', ['SVC', 'RandomForestClassifier'])\n",
    "\n",
    "    # 選ばれた分類器で分岐する\n",
    "    if classifier == 'SVC':\n",
    "        # SVC のとき\n",
    "        params = {\n",
    "            'kernel': trial.suggest_categorical('kernel', ['rbf', 'sigmoid', 'linear']),\n",
    "            'C': trial.suggest_loguniform('C', 1e+0, 1e+2),\n",
    "            'gamma': trial.suggest_loguniform('gamma', 1e-2, 1e+1),\n",
    "        }\n",
    "        model = SVC(**params)\n",
    "    else:\n",
    "        # RF のとき\n",
    "        params = {\n",
    "            'n_estimators': int(trial.suggest_int('n_estimators', 1e+2, 1e+3, log=True)),\n",
    "            'max_depth': int(trial.suggest_int('max_depth', 2, 32, log=True)),\n",
    "        }\n",
    "        model = RandomForestClassifier(**params)\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_validate(model, X=X, y=y, cv=kf, n_jobs=-1)\n",
    "    return 1.0 - scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "architectural-redhead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'classifier': 'SVC', 'kernel': 'linear', 'C': 1.0583638053977193, 'gamma': 0.012862494473148573}\n"
     ]
    }
   ],
   "source": [
    "f = partial(objective, X_train, y_train)\n",
    "study = optuna.create_study()\n",
    "study.optimize(f, n_trials=100)\n",
    "print('params:', study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "impressed-chester",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': 'linear', 'C': 1.0583638053977193, 'gamma': 0.012862494473148573}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=study.best_params\n",
    "del d['classifier']\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "moved-living",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9111111111111111"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVC(**d)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
