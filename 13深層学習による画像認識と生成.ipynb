{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, initializers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(path, s):\n",
    "    img_bgr = cv2.imread(path + '/' + s)\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    return img_rgb\n",
    "\n",
    "def read_fruits_img(name, path='C:/Users/akihiro.tanaka.CORP/Downloads/pictures/'):\n",
    "    path = path + name\n",
    " \n",
    "    list1 = os.listdir(path)\n",
    "    tmp = np.array([read_img(path, s) for s in list1])\n",
    "    tmp_gray = np.array([cv2.cvtColor(s, cv2.COLOR_RGB2GRAY) for s in tmp])\n",
    "    return tmp, tmp_gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 指定されたパスが見つかりません。: 'C:/Users/akihiro.tanaka.CORP/Downloads/images/images/shiraishi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5203ea21516e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mshiraishi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshiraishi_gray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_fruits_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'shiraishi'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C:/Users/akihiro.tanaka.CORP/Downloads/images/images/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnishino\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnishino_gray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_fruits_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nishino'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C:/Users/akihiro.tanaka.CORP/Downloads/images/images/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0makimoto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0makimoto_gray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_fruits_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'akimoto'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C:/Users/akihiro.tanaka.CORP/Downloads/images/images/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mikuta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mikuta_gray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_fruits_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ikuta'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C:/Users/akihiro.tanaka.CORP/Downloads/images/images/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_gray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_fruits_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C:/Users/akihiro.tanaka.CORP/Downloads/images/images/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-b77216f6de2b>\u001b[0m in \u001b[0;36mread_fruits_img\u001b[1;34m(name, path)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mlist1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mread_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtmp_gray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_RGB2GRAY\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 指定されたパスが見つかりません。: 'C:/Users/akihiro.tanaka.CORP/Downloads/images/images/shiraishi'"
     ]
    }
   ],
   "source": [
    "shiraishi, shiraishi_gray = read_fruits_img('shiraishi', path='C:/Users/akihiro.tanaka.CORP/Downloads/images/images/')\n",
    "nishino, nishino_gray = read_fruits_img('nishino', path='C:/Users/akihiro.tanaka.CORP/Downloads/images/images/')\n",
    "akimoto, akimoto_gray = read_fruits_img('akimoto', path='C:/Users/akihiro.tanaka.CORP/Downloads/images/images/')\n",
    "ikuta, ikuta_gray = read_fruits_img('ikuta', path='C:/Users/akihiro.tanaka.CORP/Downloads/images/images/')\n",
    "test, test_gray = read_fruits_img('test', path='C:/Users/akihiro.tanaka.CORP/Downloads/images/images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([shiraishi_gray, nishino_gray, akimoto_gray, ikuta_gray])\n",
    "X_flat = X.flatten().reshape(len(X),128**2)/255\n",
    "\n",
    "shira = np.repeat(0, len(shiraishi_gray))\n",
    "nishi = np.repeat(1, len(nishino_gray))\n",
    "aki = np.repeat(2, len(akimoto_gray))\n",
    "iku = np.repeat(3, len(ikuta_gray))\n",
    "y_label = np.concatenate([shira, nishi, aki, iku])\n",
    "y = tf.keras.utils.to_categorical(y_label, 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_flat, y)\n",
    "#X_train = X_train.reshape(len(X_train),128,128)\n",
    "#X_test = X_test.reshape(len(X_test),128,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13-1 ニューラルネットワーク\n",
    "## 13-1-1 単純パーセプトロン\n",
    "---\n",
    "### keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(4, activation='softmax', input_shape=(128*128,), name='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=256, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = model.predict(X_test)\n",
    "df = pd.DataFrame({'pred': list(map(np.argmax, p_val)),\n",
    "                   'label': list(map(np.argmax, y_test))})\n",
    "correct = df[df['pred']==df['label']]\n",
    "incorrect = df[df['pred']!=df['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 6, figsize=(16, 8), subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for i in range(4):\n",
    "    indices = list(correct[correct['pred']==i].index[:3]) + list(incorrect[incorrect['pred']==i].index[:3])\n",
    "    for j in range(6):\n",
    "        ax[i][j].imshow(X_test[indices[j]].reshape(128,128))\n",
    "        ax[i][j].set_title('pred: %d  ans: %d' % (df['label'][indices[j]], i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('correct:'+str(len(correct)))\n",
    "print('incorrect:'+str(len(incorrect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単層ニューラルネットワーク\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(1024, activation='relu', input_shape=(128*128,),\n",
    "                       kernel_initializer=initializers.TruncatedNormal(),\n",
    "                       name='hidden'))\n",
    "model.add(layers.Dense(4, activation='softmax', name='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=64, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = model.predict(X_test)\n",
    "df = pd.DataFrame({'pred': list(map(np.argmax, p_val)),\n",
    "                   'label': list(map(np.argmax, y_test))})\n",
    "correct = df[df['pred']==df['label']]\n",
    "incorrect = df[df['pred']!=df['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 6, figsize=(16, 8), subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for i in range(4):\n",
    "    indices = list(correct[correct['pred']==i].index[:3]) + list(incorrect[incorrect['pred']==i].index[:3])\n",
    "    for j in range(6):\n",
    "        ax[i][j].imshow(X_test[indices[j]].reshape(128,128))\n",
    "        ax[i][j].set_title('pred: %d  ans: %d' % (df['label'][indices[j]], i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('correct:'+str(len(correct)))\n",
    "print('incorrect:'+str(len(incorrect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.dta.DataLoader(datasets=X_train,\n",
    "                                         batch_size=10,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.a1 = nn.Sigmoid()\n",
    "        self.l2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.layers = [self.l1, self.a1, self.l2]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(128*128, 64, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optimizers.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "def compute_loss(t, y):\n",
    "    return criterion(y, torch.max(t,1)[1].long())\n",
    "\n",
    "def train_step(x, t):\n",
    "    model.train() # trainモード\n",
    "    preds = model(x)\n",
    "    loss = compute_loss(t, preds)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 20\n",
    "n_batches = X_train.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.\n",
    "    x_, t_  = shuffle(X_train, y_train)\n",
    "    x_ = torch.Tensor(x_).to(device)\n",
    "    t_ = torch.Tensor(t_).to(device)\n",
    "    \n",
    "    for n_batch in range(n_batches):\n",
    "        start = n_batch * batch_size\n",
    "        end = start + batch_size\n",
    "        loss, preds = train_step(x_[start:end], t_[start:end])\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    print('epoch: {}, loss: {:.3}'.format(epoch+1, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(x, t):\n",
    "    x = torch.Tensor(x).to(device)\n",
    "    t = torch.Tensor(t).to(device)\n",
    "    model.eval()\n",
    "    preds = model(x)\n",
    "    loss = compute_loss(t, preds)\n",
    "    return loss, preds\n",
    "\n",
    "loss, preds = test_step(X_test, y_test)\n",
    "test_loss = loss.item()\n",
    "test_acc = accuracy_score(np.argmax(y_test, axis=1).tolist(), preds.argmax(dim=-1).tolist())\n",
    "print('loss: ', test_loss)\n",
    "print('acc: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多層ニューラルネットワーク\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(1024, activation='relu', input_shape=(128*128,),\n",
    "                       kernel_initializer=initializers.TruncatedNormal(),\n",
    "                       name='hidden1'))\n",
    "model.add(layers.Dense(256, activation='relu', input_shape=(128*128,),\n",
    "                       kernel_initializer=initializers.TruncatedNormal(),\n",
    "                       name='hidden2'))\n",
    "model.add(layers.Dense(4, activation='softmax', name='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=256, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = model.predict(X_test)\n",
    "df = pd.DataFrame({'pred': list(map(np.argmax, p_val)),\n",
    "                   'label': list(map(np.argmax, y_test))})\n",
    "correct = df[df['pred']==df['label']]\n",
    "incorrect = df[df['pred']!=df['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('correct:'+str(len(correct)))\n",
    "print('incorrect:'+str(len(incorrect)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13-2 深層学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 畳み込みフィルタ\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Reshape((128, 128, 1), input_shape=(128*128,), name='rehsape'))\n",
    "model.add(layers.Conv2D(16, (9, 9), padding='same', kernel_initializer=initializers.TruncatedNormal(),\n",
    "                        use_bias=True, activation='relu',\n",
    "                        name='conv_filter'))\n",
    "model.add(layers.MaxPooling2D((2, 2), name='max_pooling'))\n",
    "model.add(layers.Flatten(name='flatten'))\n",
    "model.add(layers.Dense(1024, activation='relu', kernel_initializer=initializers.TruncatedNormal(), name='hidden'))\n",
    "model.add(layers.Dense(4, activation='softmax', name='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = model.predict(X_test)\n",
    "df = pd.DataFrame({'pred': list(map(np.argmax, p_val)),\n",
    "                   'label': list(map(np.argmax, y_test))})\n",
    "correct = df[df['pred']==df['label']]\n",
    "incorrect = df[df['pred']!=df['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('correct:'+str(len(correct)))\n",
    "print('incorrect:'+str(len(incorrect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [model.get_layer('conv_filter').output,\n",
    "                 model.get_layer('max_pooling').output]\n",
    "model2 = models.Model(inputs=model.input, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_outputs, pool_output = model2.predict(X_test[:4])\n",
    "filter_vals = model.get_layer('conv_filter').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(16, 5, figsize=(8, 20), subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for i in range(16):\n",
    "    ax[i][0].imshow(filter_vals[:,:,0,i])\n",
    "    for j in range(1,5):\n",
    "        ax[i][j].imshow(conv_outputs[j][:,:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Reshape((128,128,1), input_shape=(128*128,),name='reshape'))\n",
    "model.add(layers.Conv2D(32, (5, 5), padding='same',\n",
    "                        kernel_initializer=initializers.TruncatedNormal(),\n",
    "                        use_bias=True, activation='relu',\n",
    "                        name='conv_filter1'))\n",
    "model.add(layers.MaxPooling2D((2, 2), name='max_pooling1'))\n",
    "model.add(layers.Conv2D(64, (5, 5), padding='same',\n",
    "                        kernel_initializer=initializers.TruncatedNormal(),\n",
    "                        use_bias=True,\n",
    "                        activation='relu',\n",
    "                        name='conv_filter2'))\n",
    "model.add(layers.MaxPooling2D((2, 2), name='max_pooling2'))\n",
    "model.add(layers.Flatten(name='flatten'))\n",
    "model.add(layers.Dropout(rate=0.5, name='dropout'))\n",
    "model.add(layers.Dense(4, activation='softmax', name='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=128, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = model.predict(X_test)\n",
    "df = pd.DataFrame({'pred': list(map(np.argmax, p_val)),\n",
    "                   'label': list(map(np.argmax, y_test))})\n",
    "correct = df[df['pred']==df['label']]\n",
    "incorrect = df[df['pred']!=df['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('correct:'+str(len(correct)))\n",
    "print('incorrect:'+str(len(incorrect)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの拡張"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([shiraishi, nishino, akimoto, ikuta])\n",
    "X_flat = X.flatten().reshape(len(X),128**2*3)\n",
    "\n",
    "shira = np.repeat(0, len(shiraishi_gray))\n",
    "nishi = np.repeat(1, len(nishino_gray))\n",
    "aki = np.repeat(2, len(akimoto_gray))\n",
    "iku = np.repeat(3, len(ikuta_gray))\n",
    "y = np.concatenate([shira, nishi, aki, iku])\n",
    "y = tf.keras.utils.to_categorical(y, 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_flat, y)\n",
    "X_train = X_train.reshape(len(X_train),128,128,3)\n",
    "X_test = X_test.reshape(len(X_test),128,128,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=[0.8, 1.2],\n",
    "    horizontal_flip=True,\n",
    "    channel_shift_range=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 6, figsize=(12, 8), subplot_kw={'xticks': (), 'yticks': ()})\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(len(y_train)):\n",
    "        if np.argmax(y_train[j])==i:\n",
    "            break\n",
    "    ax[i][0].imshow(X_train[j])\n",
    "    ax[i][0].set_xticks([])\n",
    "    ax[i][0].set_yticks([])\n",
    "    for k in range(5):\n",
    "        img = datagen.flow(np.array([X_train[j]]), batch_size=1)[0][0]\n",
    "        img = img.astype('uint8')\n",
    "        ax[i][k+1].imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カラー画像の分類\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3,3), padding='same',\n",
    "                       kernel_initializer=initializers.TruncatedNormal(),\n",
    "                       use_bias=True, activation='relu',\n",
    "                       input_shape=(128, 128, 3),\n",
    "                       name='conv_filter1-1'))\n",
    "model.add(layers.Conv2D(32, (3, 3), padding='same',\n",
    "                       kernel_initializer=initializers.TruncatedNormal(),\n",
    "                       use_bias=True, activation='relu',\n",
    "                       name='conv_filter1-2'))\n",
    "model.add(layers.MaxPooling2D((2, 2), name='max_pooling1'))\n",
    "model.add(layers.Dropout(rate=0.25, name='dropout1'))\n",
    "model.add(layers.Conv2D(64, (3, 3), padding='same',\n",
    "                       kernel_initializer=initializers.TruncatedNormal(),\n",
    "                       use_bias=True, activation='relu',\n",
    "                       name='conv_filter2-1'))\n",
    "model.add(layers.Conv2D(64, (3, 3), padding='same',\n",
    "                       kernel_initializer=initializers.TruncatedNormal(),\n",
    "                       use_bias=True, activation='relu',\n",
    "                       name='conv_filter2-2'))\n",
    "model.add(layers.MaxPooling2D((2, 2), name='max_pooling2'))\n",
    "model.add(layers.Dropout(rate=0.25, name='dropout2'))\n",
    "model.add(layers.Flatten(name='flatten'))\n",
    "model.add(layers.Dense(512, activation='relu',\n",
    "                      kernel_initializer=initializers.TruncatedNormal(),\n",
    "                      name='hidden'))\n",
    "model.add(layers.Dropout(rate=0.5, name='dropout3'))\n",
    "model.add(layers.Dense(4, activation='softmax', name='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss',\n",
    "                   patience=5,\n",
    "                  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "history = model.fit_generator(\n",
    "    datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "    validation_data = (X_test, y_test),\n",
    "    steps_per_epoch=len(X_train) / batch_size,\n",
    "    epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=128, epochs=30,\n",
    "                    verbose=2,\n",
    "                    callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = model.predict(X_test)\n",
    "df = pd.DataFrame({'pred': list(map(np.argmax, p_val)),\n",
    "                   'label': list(map(np.argmax, y_test))})\n",
    "correct = df[df['pred']==df['label']]\n",
    "incorrect = df[df['pred']!=df['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 6, figsize=(16, 8), subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for i in range(4):\n",
    "    indices = list(correct[correct['pred']==i].index[1:4]) + list(incorrect[incorrect['pred']==i].index[:3])\n",
    "    for j in range(6):\n",
    "        ax[i][j].imshow(X_test[indices[j]])\n",
    "        ax[i][j].set_title('pred: %d  ans: %d' % (df['label'][indices[j]], i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('correct:'+str(len(correct)))\n",
    "print('incorrect:'+str(len(incorrect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "128*128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13-3 CNNによる画像認識と画像生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### オートエンコーダによるアノマリー検知"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128*128, activation='relu', input_shape=(128*128,)))\n",
    "#model.add(layers.Dense(512, activation='relu'))\n",
    "#model.add(layers.Dense(256, activation='relu'))\n",
    "#model.add(layers.Dense(512, activation='relu'))\n",
    "#model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Dense(128*128, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train[np.argmax(y_train, axis=1)==1], X_train[np.argmax(y_train, axis=1)==1], batch_size=256, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 6, figsize=(16, 6), subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for i in range(6):\n",
    "    ax[0][i].imshow(X_test[i].reshape((128,128)))\n",
    "    ax[1][i].imshow(model.predict(X_test[i]).reshape((128,128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model.predict(X_test)[10].reshape((128,128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
