{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-mayor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "breathing-timing",
   "metadata": {},
   "source": [
    "## 27. GANのおける工夫"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-cookie",
   "metadata": {},
   "source": [
    "### 評価指標\n",
    "最もよく使われ受け入れられている評価指標に\n",
    "- インセプション・スコア(Inception Score: IS)\n",
    "- フレチェのインセプション距離(Frechent Inception Distance: FID)\n",
    "\n",
    "がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-actor",
   "metadata": {},
   "source": [
    "#### インセプション・スコア\n",
    "評価指標が保証すべき要求には、\n",
    "- 生成サンプルは識別できるもののように見えること。  \n",
    "それはリアルであり、データセット内に存在するなにかの物体を含んで生成されること。  \n",
    "さらに画像分類器は、入力される画像内に認識できるものがあれば確実に見つけられること。  \n",
    "画像が特定のクラスに属することを確信度と共に出力できる分類器にインセプション・ネットワークがあり、ここからスコアの名前がとられている。  \n",
    "  \n",
    "- 生成されるサンプルは、元々のデータセットに含まれるクラスを図べ手ふくんでいるのが理想。  \n",
    "クラス間のモード崩壊は望まれない。  \n",
    "  \n",
    "などがある。  \n",
    "  \n",
    "ISの計算は、以下のように単純である。  \n",
    "1. 本物と偽物の分布との間でKLダイバージェンスを計算する  \n",
    "2. このべき乗をとる\n",
    "  \n",
    "#### フレチェのインセプション距離\n",
    "もしISで実現される最低限の性能で満足してよいなら、1カテゴリだけを生成するようなものでも、  \n",
    "それがカテゴリにマッチするものでありさえすれば高性能とされる。  \n",
    "FIDは画像をインセプション・ネットワークを通しながら計算される。  \n",
    "最終的なが画像ではなく、中間表現（特徴マップや層）を比較する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-waste",
   "metadata": {},
   "source": [
    "#### 訓練における課題\n",
    "- モード崩壊  \n",
    "　いくつかのモードが生成されるサンプルに含まれなくなる。  \n",
    "- 収束の遅さ  \n",
    "- 過度な汎化  \n",
    "　モード(生成されうるサンプル)の中に、サポートが存在しないものが含まれてしまう状況。  \n",
    "  \n",
    "以下のような様々な技術によって改善することができる。  \n",
    "- ネットワークを深くしていく\n",
    "- ゲームの設定を変える  \n",
    "　・ Min-Max方式と停止基準が元論文で提案されている  \n",
    "　・ 非飽和(Non-Saturating)方式と停止基準も元論文で提案されている  \n",
    "　・ Wassertein GANという改善策もある\n",
    "- 論争はあるが、訓練に使えるいくつかのハックがある  \n",
    "　・ 入力の正規化  \n",
    "　・ 勾配の制約  \n",
    "　・ 識別器をより多く訓練する  \n",
    "　・ 疎な勾配を避ける  \n",
    "　・ ソフトな、あるいはノイズ付きラベルに切り替える  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-tsunami",
   "metadata": {},
   "source": [
    "#### Min-Max GAN\n",
    "GANを**min-maxゲーム**(最大の損失が最小になるように決断を行う戦略)と捉える方法。  \n",
    "識別器のスコア関数は、\n",
    "$$\n",
    "J^D=E_{x\\sim p_r}\\log{[D(x)]}+E_{x\\sim p_g}\\log{[1-D(G(x))]}\n",
    "$$\n",
    "\n",
    "$E$は$x$(本物のデータ分布)または$z$(潜在空間)全体にわたる期待値を表す。  \n",
    "$D$は識別器を表す関数、$G$は生成器を表す関数である。  \n",
    "上の式は、二値分類問題においてよく使われる形態であり、この式を自由に単純化すると次のようになる。  \n",
    "$$\n",
    "J^D=D(x)-D(G(x)),\\ for\\ D(x),D(G(x))\\in [0,1]\n",
    "$$\n",
    "\n",
    "この式は、識別器が、正しいサンプルを間違いだと誤解する可能性(最初の項)や、  \n",
    "偽のサンプルを本物だと間違う可能性(2番目の項)を最小化しようとしていることを示している。  \n",
    "  \n",
    "生成器の損失関数は、次のようになる。  \n",
    "$$\n",
    "J_G=-J^D\n",
    "$$\n",
    "  \n",
    "#### 非飽和GAN(NS-GAN)\n",
    "このNS-GANでは、2つの損失関数が直接対抗するものとして定式化されずに、2つの独立した損失関数として定義される。  \n",
    "$$\n",
    "J^G=E_{z\\sim p_g}\\log{[D(G(x))]}\\\\\n",
    "J^D=E_{x\\sim p_r}\\log{[D(x)]}+E_{z\\sim p_g}\\log{[1-D(G(x))]}\n",
    "$$\n",
    "\n",
    "NS-GANはMM-GANなどと比べ初期の訓練が速く進むという特徴を持つ。  \n",
    "ただし、なぜうまく収束するかは理論的には説明できない。  \n",
    "  \n",
    "#### Wassertein GAN\n",
    "- 損失関数を大幅に改善し、理解可能で明確な停止条件を与えてくれる\n",
    "- WGANは実験的にもよりよい結果を出力する傾向がある\n",
    "- GANに関するこれまでの研究と違い、損失関数から話題が始まって、なぜKLダイバージェンスが理論的・実験的な観点から  \n",
    "うまく正当化できていないかを示している。  \n",
    "  \n",
    "WGANは生成サンプルの見た目の品質と明確な相関のあるearth mover's distanceを損失関数としている。  \n",
    "識別器のWassertein損失関数は、\n",
    "$$\n",
    "\\max E_{x\\sim P_r}[f_w(x)]-E_{z\\sim p(z)}[f_w(g_{\\theta}(z))]\n",
    "$$\n",
    "\n",
    "となる。  \n",
    "ここでは、識別器として働く関数$(f_w)$がある。  \n",
    "鑑定士はearth mover's distanceを推定し、これは関数$f_w$のこれまでとは異なったパラメータ化のもとで、  \n",
    "本物(最初の項)と、生成されたもの(2番目の項)の分布間の最大損失を計算している。  \n",
    "鑑定士は、$f_w$を使って2つのパラメータ空間を共有空間に投影し、生成器が動かさなければならない確率質量が  \n",
    "できるだけ大きくなるように最適化を行う。  \n",
    "生成器を表す式は、\n",
    "$$\n",
    "\\min E_{x\\sim P_r}[f_w(x)]-E_{z\\sim p_z}[f_w(g_{\\theta}(z))]\n",
    "$$\n",
    "  \n",
    "であり、本物の分布である期待値と、生成された分布である期待値の間の距離を最小化しようとしている。  \n",
    "  \n",
    "生成器が解こうとする問題は、以下のものである。  \n",
    "1. 真の分布$x$を($x\\sim P_r$)、または生成された分布($z\\sim p(z)$下で定義される$g_{\\theta}(z)$)から$x^*$を得る\n",
    "2. 潜在空間からサンプルされた$z$は$g_{\\theta}$によって変換され、$x$と同じ空間サンプル$x^*$となる  \n",
    "これは$f_w$を使って評価される   \n",
    "3. 損失関数(この場合はearth mover's distanceを使った損失関数)を最小化する  \n",
    "  \n",
    "WGANには2つの実践的意義がある。  \n",
    "- このGANには明確な停止基準を持っている。  \n",
    "それは、識別器の損失関数と見た目の品質の間に相関があることが最近の論文で示されている。  \n",
    "Wasserstein距離を測れば、いつ停止するかについて教えてくれる。  \n",
    "- WGANが収束するように訓練を行うことができる。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-exhibition",
   "metadata": {},
   "source": [
    "#### 訓練上の工夫\n",
    "#### 入力の正規化\n",
    "画素値を-1から1の間の値に正規化すると、その他の機械学習と同じように計算を容易に行うことができる。  \n",
    "  \n",
    "#### バッチ正規化\n",
    "ミニバッチを、各層へ入力するたびに正規化する。  \n",
    "  \n",
    "#### 勾配制約\n",
    "勾配が大きすぎる場合は、何か良くないことが起こっているという考え方に基づいている。  \n",
    "  \n",
    "#### 識別器を多く訓練する\n",
    "2つのやり方がある。  \n",
    "- 生成器が何も生成しないうちに、識別器をよく学習させておく\n",
    "- 訓練サイクル内で識別器をより多く更新する。  \n",
    "一般的には、生成器の更新1回あたり、識別器は5回更新する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-victim",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-adapter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
